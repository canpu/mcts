{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d807736505668a01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "import time\n",
    "import unittest\n",
    "\n",
    "from node  import Node\n",
    "from mcts  import MonteCarloSearchTree\n",
    "from mcts  import execute_round, default_rollout_policy\n",
    "from state import AbstractState  as State\n",
    "from state import AbstractAction as Action\n",
    "from test  import *\n",
    "\n",
    "from gomoku_example import *\n",
    "from maze_unfinished import *\n",
    "from maze_example    import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a00186f3414d78b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Mini-Pset: Monte Carlo Tree Search for Collaboration\n",
    "<img src=\"img/random-xkcd.png\"/>\n",
    "\n",
    "1. [Part 0: Background Information](#part-0)\n",
    "  1. [Structure of the Mini-Pset](#structure)\n",
    "  1. [Breif History](#history)\n",
    "  1. [MCTS Algorithm](#mcts-alg)\n",
    "1. [Part 1: Introduction to MCTS](#part-1)\n",
    "  1. [Provided Classes](#part-1-provided-classes) \n",
    "  1. [Implement: `select` (10 points)](#selection)\n",
    "  1. [Implement: `expand` (10 points)](#expansion)\n",
    "  1. [Implement: `default_rollout_policy` (10 points)](#rollout-policy)\n",
    "  1. [Implement: `backpropagate` (10 points)](#backpropagate)\n",
    "  1. [Written Question: *MCTS with Heuristics* (10 points)](#heuristics)\n",
    "1. [Part 2: MCTS for Collaboration](#part-2)\n",
    "  1. [Provided Classes](#part-2-provided-classes) \n",
    "  1. [Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each)](#maze)\n",
    "  1. [Written Question: *Optimal Strategy* (5 points)](#written-optimal)\n",
    "  1. [Written Question: *Maze Comparison* (5 points)](#written-comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffd73cbbb6476b5a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 0: Background Information <a id=\"part-0\"/>\n",
    "\n",
    "## Structure of the Mini-Pset <a id=\"structure\"/>\n",
    "This mini-pset is designed to give you an introduction to Monte Carlo Tree Search (MTCS) as a general planning technique as well as an introduction to using MCTS in a collaborative multi-agent scenario. The grading breakdown for this mini-pset can be seen in the outline above.\n",
    "\n",
    "**Part 0:** provides an overview of the mini-pset, along with helpful background information regarding MCTS. The MCTS algorithm itself is described in the [MCTS Algorithm](#mcts-alg) section. Provided code for completing Part 1 and Part 2 can be found in [part 1](#part-1-provided-classes) and [part 2](#part-2-provided-classes), respectively.\n",
    "\n",
    "**Part 1:** focuses on introducing the core mechanics of MCTS: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*.  This is achieved by implementing each of these core methods and then exploring the results of MCTS when applied to [Gomoku](https://en.wikipedia.org/wiki/Gomoku).\n",
    "\n",
    "**Part 2:** focuses on introducing the key sub-routines that allow the MCTS functions from part 1 to be used in a collaborative multi-agent scenario. This is done by applying MCTS to a multi-agent value collection scenario that is inspired by the Kolumbo mission. \n",
    "\n",
    "---\n",
    "## Breif History <a id=\"history\"/>\n",
    "TODO -- Finish Description Here\n",
    "\n",
    "### *\"Monte Carlo\"* meets *\"Tree Search\"*\n",
    "TODO -- Finish Description Here\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-pi-estimate.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"img/random-tree-search.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "### Bandit Problems\n",
    "<img src=\"img/random-bandit-cartoon.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Bandit Problems* are a class of sequential decision making problems where an agent must chose between $K$ actions in order to maximize the cumulative reward of the agent for some amount of time horizon.  For example, the agent (the bandit) in the scenario shown in the image above is an octopus, and the possible actions available to the octopus at any point in time correspond to the available slot machine levers. The goal of the octopus is to devise an optimal policy via experimentation because the underlying reward distribution associated with each lever is unknown. This means that the octopus must estimate the potential reward of pulling a particular lever based on the past observations of taking this action. This introduces the **exploitation-exploration dilemma**: the bandit needs to take advantage of the knowledge it has already gained about the system to make choices associated with high rewards, but the bandit must also must consider unexplored actions to learn more about the underlying reward distribution of said action. \n",
    "\n",
    "The best action is not clear because the underlying reward distribution is unknown, and potential rewards must be estimate on past observations. This leads to the exploitation-exploration dilemma. [1] The policy should aim to minimize a player's regret after *n* plays, which is defined as $R_n$ below:\n",
    "\n",
    "$$R_n = \\mu^*n - \\mu_j \\Sigma_{j=1}^{K} \\mathbb{E}[T_k(n)]$$\n",
    "\n",
    "where $\\mu^*$ is the best possible expected reward and $\\mathbb{E}[T_j(n)]$ is the expected number of executions of action $j$ across $n$ trials [1]. This leads us to the notion of an Upper Confidence Bound (UCB) that a particular action is optimal because UCB suggests a policy to follow in order to minimize the growth of regret for the bandit over time.\n",
    "\n",
    "### Upper Confidence Bound (UCB)\n",
    "TODO -- Finish Description Here\n",
    "\n",
    "A useful notion for tackling bandit problems is the upper confidence bound (UCB) that a particular action is the optimal action. \n",
    "\n",
    "$$UCB1 = \\bar{X_j} + \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$ -> first term encourages exploitation (avg reward from arm j), and second encourages exploration (visit less visited choices) [TODO find source for this (see Auer et al)]\n",
    "\n",
    "\n",
    "---\n",
    "## MCTS Algorithm <a id=\"mcts-alg\">\n",
    "TODO -- Finish Description\n",
    "<img src=\"img/paper-mcts.png\" style=\"width: 700px;\" >\n",
    "\n",
    "TODO -- Finish Description\n",
    "\n",
    "two fundamental concepts: the value of an action may be approximated by using random simulation, and that these values may be used efficient to adjust the policy towards a best-first strategy \n",
    " \n",
    "<img src=\"img/paper-tree-growth.png\" style=\"width: 400px;\" >\n",
    "\n",
    "### Upper Confidence Bound for Trees (UCT)\n",
    "TODO -- Finish Description\n",
    "\n",
    "UCT refers to MCTS with any UCB tree selection policy. We will encode UCT with UCB1 in this problem set.\n",
    "$$UCT = \\bar{X_j} + 2 C_p \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$ (n is how many times the parent node has been visited) -> yields a powerfu form of iterated local search. exploration gaurantees that low-value children are to be chosen eventually. When rewards are in the range [0, 1], Cp optimal = 0.5. \n",
    "\n",
    "### Default Policy: Random Rollout \n",
    "TODO -- Finish Description\n",
    "\n",
    "<img src=\"img/random-tree-rollout.png\" style=\"width: 300px;\" >\n",
    "\n",
    "### Fun Example: AlphaGo Zero\n",
    "TODO -- Finish Description Here\n",
    "\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-go-performance.gif\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/random-lee-sedol.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-744c8c9c22888a18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " ---\n",
    "# Part 1: Introduction to MCTS <a id=\"part-1\"/>\n",
    "\n",
    "In the first part, you will be asked to finish the MCTS algorithm, where necessary utility functions and classes have been created but the four key steps are still waiting your implementation. After validating your code, you can see how your algorithm helps build a Gomoku AI!\n",
    "\n",
    "We have provided a Gomoku game which is steill in progress, as shown below. The black is at great advantage, and your job is to help it turn advantage into victory.\n",
    "\n",
    "<img src=\"img/gomoku_example_initial_state.png\" style=\"width: 400px;\" >\n",
    "\n",
    "---\n",
    "## Provided Classes <a id=\"part-1-provided-classes\"/>\n",
    "\n",
    "\n",
    "You are now asked to finish several key components in Monte Carlo Tree Search, namely the `select`, `expand`, `default_rollout_policy` and `backpropagate` functions. You will need to use the `Node` class we have defined.\n",
    "\n",
    "### `Node` Class\n",
    "\n",
    "A `Node` object has the following useful member variables (callable via `my_node.children`, etc):\n",
    "1. `children`: a dictionary that maps `Action` objects to `Node` objects\n",
    "1. `tot_reward`: a float of reward that has been back-propagated to this node\n",
    "1. `num_samples`: an int of number of samples that has been back-propagated to this node\n",
    "\n",
    "the following properties (callable via `my_node.state`, etc):\n",
    "1. `state`: a `State` objects corresponds to the node\n",
    "1. `parent`: a `Node` object that is the parent\n",
    "1. `unused_edges`: a list of `Action` objects that have not contributed to building child nodes\n",
    "1. `is_terminal`: a bool representing whether or not the `State` object associated with the node is a terminal state\n",
    "1. `depth`: an int of the depth of the node. The depth of a root node, which has `None` as its parent, is 1.\n",
    "1. `is_expanded`: a bool representing whether all possible actions and associated child nodes have been added to the `children` dictionary\n",
    "\n",
    "and the following methods:\n",
    "1. `add_child`(`action`): if the action is in the list `unused_edges`, then build a `Node` object `child` and add `action`: `child` to `children`, and returns `child`\n",
    "\n",
    "When making random selection from a list of variables, use the `random.choice()` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1c1dc82cdbe3ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `select` (10 points) <a id=\"selection\"/>\n",
    "<img src=\"img/paper-selection.png\" style=\"width: 700px;\" >\n",
    "\n",
    "\n",
    "Selecting which node to expand is based on choosing the node with the maximal upper confidence bound (UCB):\n",
    "$$UCB=\\frac{\\text{reward}}{\\text{number of samples of the child node}}+\\sqrt{2\\frac{\\ln({\\text{number of samples of the parent node})}}{\\text{number of samples of the child node}}}$$\n",
    "where the first term is the average reward per sample and is the exploitation term; the second term, coming from Hoeffding's inequality, will favor less unexplored choices and is the exploration term.\n",
    "\n",
    "Later when we are choosing the best strategy (not in simulation), we need to choose the action that brings the maximal reward instead of UCB, so a good practice in implementation is to make the second term vanishable:\n",
    "$$UCB=\\frac{\\text{reward}}{\\text{number of samples of the child node}}+exploration\\_const*\\sqrt{2\\frac{\\ln({\\text{number of samples of the parent node})}}{\\text{number of samples of the child node}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5146225d8db989b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select(node: Node, exploration_const: float = 1.0) -> (Action, Node):\n",
    "    \"\"\" Select the best child node based on UCB; if there are multiple\n",
    "        child nodes with the same max UCB, randomly select one\n",
    "    :param node: The parent node\n",
    "    :param exploration_const: The exploration constant in UCB formula\n",
    "    :return: The action and the corresponding child node it leads to\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    best_ucb = -math.inf\n",
    "    best_actions = []\n",
    "    for action, child in node.children.items():\n",
    "        \n",
    "        for action, child in node.children.items():\n",
    "            node_ucb = (child.tot_reward / child.num_samples\n",
    "                        + exploration_const *\n",
    "                        math.sqrt(2.0 * math.log(node.num_samples) /\n",
    "                                  child.num_samples))\n",
    "            if node_ucb > best_ucb:\n",
    "                best_ucb = node_ucb\n",
    "                best_actions = [action]\n",
    "            elif node_ucb == best_ucb:\n",
    "                best_actions.append(action)\n",
    "        \n",
    "    best_action = random.choice(best_actions)\n",
    "    return best_action, node.children[best_action]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1728113dfcfcc33f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_select(select)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `expand` (10 points) <a id=\"expansion\"/>\n",
    "<img src=\"img/paper-expansion.png\" style=\"width: 700px;\" >\n",
    "Suppose you have selected a node and that node is not a terminal node, you need to randomly select an action from the list of available untaken actions, add the associated new child node to the *children* dictionary and return the child node.\n",
    "\n",
    "Hint: use the functions provided above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cac40d673a2ac43",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def expand(node: Node) -> Node:\n",
    "    \"\"\" Randomly select an untried action and create a child node based on it\n",
    "        Return the new child node\n",
    "    :param node: The parent node\n",
    "    :return: The child node\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    if node.is_expanded:\n",
    "        raise Exception(\"Should not expand a node that has already\"\n",
    "                        \" been expanded\")\n",
    "        \n",
    "    # Insert your code here\n",
    "    action = random.choice(node.unused_edges)\n",
    "    return node.add_child(action)\n",
    "    ### BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d6f102ff29b07b2b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_expand(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `default_rollout_policy` (10 points) <a id=\"rollout-policy\"/>\n",
    "<img src=\"img/paper-simulation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "\n",
    "The default rollout policy is to keep randomly selecting an action from the available ones until a terminal state is reached.\n",
    "A State object has a property is_terminal to represent whether a it is a terminal state, another property possible_actions which is a list of Action objects, and a member method execute_action which takes an Action object as input and returns a copy of a new state.\n",
    "Hint: use random.choice to randomly select an action from a list of actions.\n",
    "\n",
    "\n",
    "### Default Rollout Policy\n",
    "\n",
    "\n",
    "TODO -- Finish Description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04f2b4885acdea15",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def default_rollout_policy(state: State) -> float:\n",
    "    \"\"\" The default policy for simulation is to randomly (uniform distribution)\n",
    "        select an action to update the state and repeat the simulation until\n",
    "        a terminal state is reached\n",
    "    :param state: The starting state\n",
    "    :return: The reward at the terminal state\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while not state.is_terminal:\n",
    "        action = random.choice(state.possible_actions)\n",
    "        state = state.execute_action(action)\n",
    "    return state.reward\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c3db0bef45352e1",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_default_rollout_policy(default_rollout_policy)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `backpropagate` (10 points) <a id=\"backpropagate\"/>\n",
    "<img src=\"img/paper-back-propagation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "\n",
    "After simulation is performed, the next thing is to update the visit counts and rewards for relevant nodes.\n",
    "Hint: Node objects have a member property parent; older visit count and reward are stored in public member variables num_samples and tot_reward respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20127172b9eb7705",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backpropagate(node: Node, reward: float = 0.0) -> None:\n",
    "    \"\"\" Propagate the reward and sample count from the specified node\n",
    "        back all the way to the root node (the node with None as parent)\n",
    "    :param node: The node wheresimulation is performed and\n",
    "        the reward is evaluated\n",
    "    :param reward: The reward at the node\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while node is not None:\n",
    "        node.num_samples += 1\n",
    "        node.tot_reward += reward\n",
    "        node = node.parent\n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e77f09e169ac4a3e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_backpropagate(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Written Question: *MCTS with Heuristics* (10 points) <a id=\"heuristics\"/>\n",
    "\n",
    "Now you have implemented the MCTS algorithm, you should be ready to use it run it on the Gomoku problem. Note that the execution might take 3-5 minutes. To ensure that the simulation always terminates in a reasonable number of steps and to make sure that the results observed by everyone are similar, we have fixed the random seed.\n",
    "\n",
    "Does the result look good? Start thinking about why (you do not need to answer it right now).\n",
    "\n",
    "Which strategy generates a better result? Explain why. Could you come up with some potential methods to improve the performance of MCTS in Gomoku games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f3031d5ac29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate_with_black_sample_arbitrarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonteCarloSearchTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36msimulate_with_black_sample_arbitrarily\u001b[0;34m(mcts_class)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     (gomoku_example_simulate(mcts_class, black_heuristics=False,\n\u001b[0;32m---> 71\u001b[0;31m                              white_heuristics=True, seed=1000).visualize())\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36mgomoku_example_simulate\u001b[0;34m(mcts_class, black_heuristics, white_heuristics, samples_per_step, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblack_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Black goes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mblack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_for_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/mcts.py\u001b[0m in \u001b[0;36msearch_for_actions\u001b[0;34m(self, search_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mexecute_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_tree_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcur_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/mcts.py\u001b[0m in \u001b[0;36mexecute_round\u001b[0;34m(root, rollout_policy, max_tree_depth)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_expanded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0msimulation_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_tree_depth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expand' is not defined"
     ]
    }
   ],
   "source": [
    "simulate_with_black_sample_arbitrarily(MonteCarloSearchTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1051e1007af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate_with_black_sample_neighborhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonteCarloSearchTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36msimulate_with_black_sample_neighborhood\u001b[0;34m(mcts_class)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \"\"\"\n\u001b[1;32m     78\u001b[0m     (gomoku_example_simulate(mcts_class, black_heuristics=True,\n\u001b[0;32m---> 79\u001b[0;31m                              white_heuristics=True, seed=500).visualize())\n\u001b[0m",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36mgomoku_example_simulate\u001b[0;34m(mcts_class, black_heuristics, white_heuristics, samples_per_step, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblack_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Black goes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mblack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_for_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/mcts.py\u001b[0m in \u001b[0;36msearch_for_actions\u001b[0;34m(self, search_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mexecute_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_tree_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcur_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/mcts.py\u001b[0m in \u001b[0;36mexecute_round\u001b[0;34m(root, rollout_policy, max_tree_depth)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_expanded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0msimulation_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_tree_depth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expand' is not defined"
     ]
    }
   ],
   "source": [
    "simulate_with_black_sample_neighborhood(MonteCarloSearchTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d4021b380fffb2a4",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: MCTS for Collaboration <a id=\"part-2\"/>\n",
    "<img src=\"img/auv-random-walk.png\" style=\"width: 400px;\" >\n",
    "\n",
    "\n",
    "In part 1, you have implemented the MCTS algorithm; in part 2, you will learn how to model a problem which can call that algorithm\n",
    "\n",
    "---\n",
    "## AUV Reward Collection Problem\n",
    "Suppose you are given 3 AUVs and your task is to use them to explore valuable locations of an underwater area, which has been spatially discretized below:\n",
    "\n",
    "<img src=\"img/maze_example_1.png\" style=\"width: 600px;\" >\n",
    "\n",
    "The time is also discretized into 15 steps. At each time step, an AUV can move to one of the 4 neighboring locations as long as it is not an obstacle (the red tiles in the figure). The radius of blue circles represent the information values of different locations: the larger radius corresponds to 3, and the smaller represents 1.\n",
    "\n",
    "We refer to this example as Maze Example 1. The task is to model this problem so the algorithm developed in Part 1 can be applied (and you will know how we implemented the Gomoku game because they are quite similar).\n",
    "\n",
    "\n",
    "---\n",
    "## Provided Classes <a id=\"part-2-provided-classes\"/>\n",
    "\n",
    "We have provided a class `UnfinishedMazaState`, and your job is to finish building the class `MazeState`. To be more specific, you need to implement 4 key properties/methods, namely `is_terminal`, `reward`, `possible_actions` and `take_action`. You may find the following information useful.\n",
    "\n",
    "### `MazeAction` Class\n",
    "1. Can be constructed using `MazeAction(agent_index: int, position: (int, int)`, where `agent_index` indicates which AUV should move, and position is a tuple representing where it is moving to\n",
    "1. Has properties `agent_index` and `position`\n",
    "\n",
    "### `MazeEnvironment` Class\n",
    "1. Has property `rewards`, whcih is a dictionary mapping position (int, int) to reward value (float)\n",
    "2. Has property `obstacles`, which is a set of position (int, int)\n",
    "\n",
    "### `UnfinishedMazeState` Class\n",
    "1. Has a method `is_in_range` that takes (int, int) as input and returns a bool, indicating whether a position is inside the problem domain\n",
    "1. Has a property `paths` - \\[`path_0`, `path_1`, `path_2`, $\\ldots$\\]. Each `path_i` is a list of position tuples (int, int); the last tuple is current position of agent $i$. This records the trajectories of all agents\n",
    "1. Has a property `time_remains` which is an int object\n",
    "1. Has a property `visited` which is a set of visited position tuples (int, int)\n",
    "1. Has a property `turn` - an int object indicating which agent should move\n",
    "1. Has a property `switch_agent` which will properly update `turn` so the next agent can move; if all agents have moved for a round, then the `time_remains` property would be updated\n",
    "1. Has a property `environment`, which is a `MazeEnvironment` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each) <a id =\"maze\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(UnfinishedMazeState):\n",
    "\n",
    "    def __init__(self, environment: MazeEnvironment, time_remains: int = 10):\n",
    "        \"\"\" Create a state of the AUV reward-collection game\n",
    "        \"\"\"\n",
    "        super(UnfinishedMazeState, self).__init__(environment, time_remains)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def reward(self) -> float:\n",
    "        \"\"\" The total reward at the current state is value of\n",
    "            rewards that have been visited by agents\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        reward = 0.0\n",
    "        for target_position in self._environment.rewards:\n",
    "            if target_position in self.visited:\n",
    "                reward += self._environment.rewards[target_position]\n",
    "        return reward\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\" The only terminal condition is no time remains\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.time_remains <= 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "        \n",
    "    @property\n",
    "    def possible_actions(self) -> list:\n",
    "        \"\"\" The possible actions based on the current state\n",
    "            Note that you are only moving a single agent\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        i, j = self._paths[self._turn][-1]\n",
    "        actions = [MazeAction(self._turn, (i + 1, j)),\n",
    "                   MazeAction(self._turn, (i - 1, j)),\n",
    "                   MazeAction(self._turn, (i, j + 1)),\n",
    "                   MazeAction(self._turn, (i, j - 1))]\n",
    "        return [MazeAction(self._turn, action.position) for action in actions if\n",
    "                self.is_in_range(action.position) and\n",
    "                action.position not in self._environment.obstacles]\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    def take_action(self, action: MazeAction) -> \"MazeState\":\n",
    "        \"\"\" Execute the action based on the current state\n",
    "            You can assume that the action is always valid\n",
    "        :param action: The action\n",
    "        :return: The state after being updated\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self._paths[self._turn].append(action.position)\n",
    "        self._turn = (self._turn + 1) % len(self._paths)\n",
    "        if self._turn == 0:  # When all agents have taken a turn of actions\n",
    "            self._time_remains -= 1\n",
    "        return self\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your code is validated, you can run the simulation. Note that the simulation would take several minutes and we have fixed the random seed.\n",
    "\n",
    "You can check the check the correctness of your code in incremental steps \n",
    "TODO -- mention that tests assume order of implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_1() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-14ddfb3b1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_1() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_1()).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1315a5a1f55ef576",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_reward(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5a1a4a7e0465456",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_is_terminal(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-727963e36b39a45f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_possible_actions(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-53bbf722c18cb613",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_take_action(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Written Question: *Optimal Strategy* (5 points) <a id=\"written-optimal\"/>\n",
    "\n",
    "What is the optimal strategy? You only need to conceptually describe it. There is no need to enter the exact moves for each time steps.\n",
    "\n",
    "For example:\n",
    "> The AUV starting at (13, 8) should first collect the reward at (14, 10), (14, 11), and then move towards the reward at (15, 8)\n",
    "> The AUV starting at (7, 7) should move towards the reward at (11, 2)\n",
    "> The AUV starting at (12, 12) should move towards the reward at (4, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25cdfc53a4e11f2d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508fc0042443dd14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, run the simulation. The execution will again take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40f6bdd9ff826f63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_1() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-14ddfb3b1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_1() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_1()).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e39854823691bcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Written Question: *Maze Comparison* (5 points) <a id=\"written-comparison\"/>\n",
    "\n",
    "\n",
    "When using the default rollout policy, do you think MCTS will perform equally well in Maze Example 2 as in Maze Example 1? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ae33ce4e0c56bfb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_2() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-eef44224e8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_2() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_2()).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b07cc8047b2ed26e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af0a8bf29b28875b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## References \n",
    "[1] Browne et. al. 2012, *A Survey of Monte Carlo Tree Search Methods*, IEEE\n",
    "\n",
    "TODO -- add more references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
