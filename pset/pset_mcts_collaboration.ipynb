{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d807736505668a01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "import time\n",
    "import unittest\n",
    "\n",
    "from state import AbstractState  as State, AbstractAction as Action\n",
    "from mcts  import *\n",
    "from test  import *\n",
    "\n",
    "from gomoku_example import *\n",
    "from maze_unfinished import *\n",
    "from maze_example    import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a00186f3414d78b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Mini-Pset: Monte Carlo Tree Search for Collaboration\n",
    "<img src=\"img/random-xkcd.png\"/>\n",
    "\n",
    "1. [Part 0: Background Information](#part-0)\n",
    "  1. [Structure of the Mini-Pset](#structure)\n",
    "  1. [*Monte Carlo* meets *Tree Search*](#monte-carlo-meets-tree-search)\n",
    "  1. [MCTS Algorithm](#mcts-alg)\n",
    "1. [Part 1: Introduction to MCTS](#part-1)\n",
    "  1. [Provided Classes](#part-1-provided-classes) \n",
    "  1. [Implement: `select` (10 points)](#selection)\n",
    "  1. [Implement: `expand` (10 points)](#expansion)\n",
    "  1. [Implement: `default_rollout_policy` (10 points)](#rollout-policy)\n",
    "  1. [Implement: `backpropagate` (10 points)](#backpropagate)\n",
    "  1. [Written Question: *MCTS with Heuristics* (10 points)](#heuristics)\n",
    "1. [Part 2: MCTS for Collaboration](#part-2)\n",
    "  1. [Provided Classes](#part-2-provided-classes) \n",
    "  1. [Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each)](#maze)\n",
    "  1. [Written Question: *Optimal Strategy* (5 points)](#written-optimal)\n",
    "  1. [Written Question: *Maze Comparison* (5 points)](#written-comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffd73cbbb6476b5a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 0: Background Information <a id=\"part-0\"/>\n",
    "\n",
    "## Structure of the Mini-Pset <a id=\"structure\"/>\n",
    "This mini-pset is designed to give you an introduction to Monte Carlo Tree Search (MTCS) as a general planning technique as well as an introduction to using MCTS in a collaborative multi-agent scenario. The grading breakdown for this mini-pset can be seen in the outline above.\n",
    "\n",
    "**Part 0:** provides an overview of the mini-pset, along with helpful background information regarding MCTS. The MCTS algorithm itself is described in the [MCTS Algorithm](#mcts-alg) section. Provided code for completing Part 1 and Part 2 can be found in [Part 1](#part-1-provided-classes) and [Part 2](#part-2-provided-classes), respectively.\n",
    "\n",
    "**Part 1:** focuses on introducing the core mechanics of MCTS: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*.  This is achieved by implementing each of these core methods and then exploring the results of MCTS when applied to [Gomoku](https://en.wikipedia.org/wiki/Gomoku).\n",
    "\n",
    "**Part 2:** focuses on introducing the key sub-routines that allow the MCTS functions from part 1 to be used in a collaborative multi-agent scenario. This is done by applying MCTS to a multi-agent value collection scenario that is inspired by the Kolumbo mission. \n",
    "\n",
    "---\n",
    "## *Monte Carlo* meets *Tree Search* <a id=\"monte-carlo-meets-tree-search\"/>\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-pi-estimate.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"img/random-tree-search.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Generally speaking, *Monte Carlo* methods are a set of algorithms that use random sampling techniques to estimate the value of unknown parameters. Monte Carlo methods are particularly useful when the state space in question is large enough that brute force enumeration is infeasible. By employing the Law of Large numbers, we know that *“As the number of identically distributed, randomly generated variables increases, their sample mean approaches their theoretical mean.”* This law serves as the basis for Monte Carlo methods. As a simple example, Monte Carlo methods can be used to estimate the mathematical value Pi. This is shown in the image above.\n",
    "\n",
    "Trees on the other hand are a restricted form of a graph that allows us to model a wide variety of decision making processes. Importantly, we can employ search methods over a tree to quickly find the optimal solution, given that the tree is of reasonable size. \n",
    "\n",
    "So what happens when we combine both paradigms? One way to think about *Monte Carlo Tree Search* is that the *Monte Carlo* mechanism serves as a method for estimating the value of a particular state in the game, done commonly with a random rollout policy, and the *Tree Search* mechanism uses an exploitation-exploration tradeoff with the value estimate and the visitation count of the node to determine which node to expand next. \n",
    "\n",
    "MCTS methods perform particularly well in scenarios when the branching factor is quite high, there are no hand-constructed heuristics available, or an anytime solution is required.  \n",
    "\n",
    "### Bandit Problems\n",
    "<img src=\"img/random-bandit-cartoon.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Bandit Problems* are a class of sequential decision making problems where an agent must chose between $K$ actions in order to maximize the cumulative reward of the agent for some amount of time horizon.  For example, the agent (the bandit) in the scenario shown in the image above is an octopus, and the possible actions available to the octopus at any point in time correspond to the available slot machine levers. The goal of the octopus is to devise an optimal policy via experimentation because the underlying reward distribution associated with each lever is unknown. This means that the octopus must estimate the potential reward of pulling a particular lever based on the past observations of taking this action. This introduces the **exploitation-exploration dilemma**: the bandit needs to take advantage of the knowledge it has already gained about the system to make choices associated with high rewards, but the bandit must also must consider unexplored actions to learn more about the underlying reward distribution of said action. \n",
    "\n",
    "A useful construct for encoding the exploitation-exploration dilemma is **regret**, where regret is the expected loss for the bandit due to not acting optimally.  In this sense, an appropriate strategy for the bandit would be to minimize this regret. To express the regret of the bandit after *n* plays, defined as $R_n$ we can write: \n",
    "\n",
    "$$R_n = \\mu^*n - \\mu_j \\Sigma_{j=1}^{K} \\mathbb{E}[T_k(n)]$$\n",
    "\n",
    "where $\\mu^*$ is the best possible expected reward and $\\mathbb{E}[T_j(n)]$ is the expected number of executions of action $j$ across $n$ trials [[1]](#references). This leads us to the notion of an Upper Confidence Bound (UCB) that a particular action is optimal because UCB suggests a policy to follow in order to minimize the growth of regret for the bandit over time.\n",
    "\n",
    "### Upper Confidence Bound (UCB)\n",
    "A useful notion for tackling bandit problems is the upper confidence bound (UCB) that a particular action is the optimal action.  The UCB is useful because it allows the agent to consider both exploitation and exploration when considering what state to explore next. Perhaps the most famous example of a UCB is known as UCB1, shown below. \n",
    "\n",
    "$$\\text{UCB1} = \\bar{X_j} + \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$\n",
    "\n",
    "The first term, $\\bar{X_j}$  represents the current estimated value of action $j$, which improves over time as sampling continues. This serves the exploitation term: when the average value of a particular action is high, we would like to exploit this action in the future. The next term,  $\\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$ represents the number of times that node $j$ has been sampled relative to its siblings. Here, $n$ is the number of times that node $j$’s parent has been visited in the tree, and $n_j$ is the number of times that node $j$ has been visited. This serves as the exploration term: as the value of $n_j$ relative to $n$ decreases, the UCB value increases, and we are encouraged to explore relatively unexplored states. This version of UCB can readily be extended to include an additional weighting term between the exploration and exploitation terms. We won’t get into the math here, but to summarize, the UCB1 method takes advantage of the Chernoff-Hoeffding inequality to establish provably slow growth of regret for multi-bandit problems [[2]](#references).\n",
    "\n",
    "\n",
    "---\n",
    "## MCTS Algorithm <a id=\"mcts-alg\">\n",
    "    \n",
    "As discussed in lecture, the MCTS algorithm is a general framework that consists of four key phases: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*. During the *Selection* and *Expansion* phases, the *Tree Policy* is used, while during the *Simulation* phase, the *Default Policy* is used. The MCTS algorithm allows the *Tree Policy* and *Default Policy* to take on many forms. In this mini-pset, we will focus on using the Upper Confidence Bound for Trees (UCT) to serve as the *Tree Policy* and Random Rollout as the *Default Policy.* Both of these policies are discussed in more detail below.\n",
    "\n",
    "<img src=\"img/paper-mcts.png\" style=\"width: 700px;\" >\n",
    "\n",
    "Before we get into UCT and Random Rollout, it is important to remind ourselves of the two fundamental concepts that MCTS is build upon: the value of an action may be approximated by using random simulation, and that these values may be used efficiently to adjust the policy towards a best-first strategy. \n",
    "\n",
    "### Upper Confidence Bound for Trees (UCT)\n",
    "Generally speaking Upper Confidence Bound for Trees (UCT) refers to an MCTS algorithm that utilizes some UCB variant in the tree selection policy. In our case, we will use UCB1 with an added weighting factor as our selection policy. This means that during our selection phase, we select the node with the best UCT value. Keep in mind that the UCT term already includes both exploitation and exploration incentives. Pulling things together, we get the following equation for UCT:\n",
    "\n",
    "$$\\text{UCT} = \\bar{X_j} + 2 C_p \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$ \n",
    "\n",
    "Again $\\bar{X_j}$ is the estimated value of taking action $j$, $n$ is how times the parent node has been visited, $n_j$ is the number of times action $j$ has been taken from this state, and $C_p$ is the additional weighting factor that can toggle the relative importance of the exploitation and exploration terms. If multiple nodes are found to have the same UCT value, the tie is broken by randomly selecting one of the nodes.\n",
    "\n",
    "The UCT criteria for *Selection* and *Expansion* is what allows for asymmetric tree growth, shown in the figure below. This asymmetric tree growth is why MCTS performs better compared to brute-force exhaustive search and pruning techniques for adversarial games, such as Alpha-Beta pruning, because MCTS will focus its search on promising areas of the tree.\n",
    "\n",
    "<img src=\"img/paper-tree-growth.png\" style=\"width: 400px;\" >\n",
    "\n",
    "### Default Policy: Random Rollout \n",
    "Once an action has been selected and expanded, it is now time to perform simulation in order to retrieve an estimate for how good (or bad) this particular state is. This is where the *Default Policy* is used to get from the current state to a terminal state. Once at the terminal state, we can assess the value of this simulated version of the game and use back-propagation to share this information up the tree. As mentioned previously, we harness the power of random sampling in our *Default Policy* by employing *Random Rollout*. Random Rollout simply chooses actions randomly from the set of possible actions at a given state until a terminal state is reached. It’s that simple. It is also possible to incorporate some heuristics to guide the behavior of the rollout, but we will not consider heuristics at this time. \n",
    "\n",
    "The figure below shows an example of a random rollout for a Tic-Tac-Toe game. Since the blue circle ends up winning the game, they improve the estimated value of the first move that was made. \n",
    "\n",
    "<img src=\"img/random-tree-rollout.png\" style=\"width: 300px;\" >\n",
    "\n",
    "## Fun Example: AlphaGo Zero\n",
    "Before jumping into the implementation work of this problem set, we quickly mention a recent example of using MCTS to devise the best Go-playing AI system to date: AlphaGo Zero, to help motivate the power of MCTS. AlphaGo Zero three core components: MCTS, Neural Networks, and Reinforcement Learning. The MCTS is used in AlphaGo Zero to determine which node to expand next in the tree, just like how we will utilize MCTS in this mini-pset. However, AlphaGo Zero also uses a Neural Network to assess the value of a state and Reinforcement Learning to update the weights of this Neural Network. Also, AlphaGo Zero had the luxury of playing against itself MANY times (approximately 19.6 million self-play games), on a high-powered machine. That said, MCTS is still the workhorse that pucks out the next move to make. [[3]](#references)\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-go-performance.gif\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/random-lee-sedol.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-744c8c9c22888a18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " ---\n",
    "# Part 1: Introduction to MCTS <a id=\"part-1\"/>\n",
    "\n",
    "In Part 1 of the mini-pset, you are asked to implement the four stages of the MCTS algorithm. After validating your code for each of these stages, you can see how your algorithm helps build a Gomoku AI!\n",
    "\n",
    "The core objective of Gomoku is to get five of your tokens aligned in a row, either horizontally, vertically, or in either diagonal direction. At any given turn during the game, you are allowed to place your token in any free space. The game ends as soon as five-in-a-row is achieved somewhere on the board, and the game is typically played on a Go board. However, the specific rules regarding how to play Gomoku are not important for the mini-pset because the core mechanics of MCTS are not specific to a particular game or scenario. To support the simulation of the Gomoku game, we have provided you with `GomokuState` and `GomokuAction` classes that embody the mechanics of the Gomoku game. You will not need to interact with these classes directly. An example of the Gomoku game is shown below.\n",
    "\n",
    "<img src=\"img/gomoku_example_initial_state.png\" style=\"width: 400px;\" >\n",
    "    \n",
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## Provided Classes <a id=\"part-1-provided-classes\"/>\n",
    "\n",
    "\n",
    "You are now asked to finish several key components in Monte Carlo Tree Search, namely the `select`, `expand`, `default_rollout_policy` and `backpropagate` functions. You will need to use the `Node` class that we have provide.\n",
    "\n",
    "### `Node` Class\n",
    "\n",
    "`Node` **member variables** (accessible via `my_node.children`, etc):\n",
    "1. `children`: a dictionary that maps `Action` objects to `Node` objects, representing the children of the  node\n",
    "1. `tot_reward`: a float of reward that has been back-propagated to this node\n",
    "1. `num_samples`: an int of number of samples that has been back-propagated to this node\n",
    "\n",
    "`Node` **properties** (accessible via `my_node.state`, etc):\n",
    "1. `state`: a `State` object corresponding to state of the node\n",
    "1. `parent`: a `Node` object that is the parent\n",
    "1. `unused_edges`: a list of `Action` objects that have not contributed to building child nodes (i.e. unexplored actions from this node)\n",
    "1. `is_terminal`: a bool representing whether or not the `State` object associated with the node is a terminal state\n",
    "1. `depth`: an int of the depth of the node. The depth of a root node is 1 (the parent of the root node is `None`)\n",
    "1. `is_expanded`: a bool representing whether all possible actions and associated child nodes have been added to the `children` dictionary\n",
    "\n",
    "`Node` **methods** (accessible via `my_node.add_child(new_action)`)\n",
    "1. `add_child(action)`: if `action` is in the list `unused_edges`, then this method builds a new `Node` object called `child`, adds the key-value pair `action`:`child` to the `children` dictionary, and returns returns `child`\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Hints\n",
    "- when making random selection from a list of variables, use the `random.choice()` function\n",
    "- when evalauting the natural log, you can use `math.log()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1c1dc82cdbe3ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `select` (10 points) <a id=\"selection\"/>\n",
    "<img src=\"img/paper-selection.png\" style=\"width: 700px;\" >\n",
    "\n",
    "Selecting a node to expand is based on choosing the node with the maximal upper confidence bound (UCB):\n",
    "$$UCB=\\left(\\frac{\\text{total_reward}}{\\text{number_of_samples_of_the_child_node}}\\right)+\\sqrt{2\\frac{\\ln({\\text{number_of_samples_of_the_parent_node})}}{\\text{number_of_samples_of_the child_node}}}$$\n",
    "where the first term is the average reward per sample (the exploitation term), and the second term, coming from Hoeffding's inequality, will favor less unexplored choices (the exploration term).\n",
    "\n",
    "Later when we are choosing the best strategy for the agent (we are actually taking an action in real-life rather than simulating actions in MCTS), we pursue pure exploitation. This means that we need a way to toggle on and off the exploration term. To achieve this, we add an additional term called the “exploration_constant” to the equation, giving us the resulting definition of UCB:\n",
    "\n",
    "$$UCB=\\left(\\frac{\\text{total_reward}}{\\text{number_of_samples_of_the_child_node}}\\right)+\\text{exploration_constant}*\\sqrt{2\\frac{\\ln({\\text{number_of_samples_of_the_parent_node})}}{\\text{number_of_samples_of_the child_node}}}$$\n",
    "\n",
    "Remember to used the [Provided Classes](#part-1-provided-classes) when implementing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5146225d8db989b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select(node: Node, exploration_const: float = 1.0) -> (Action, Node):\n",
    "    \"\"\" Select the best child node based on UCB; if there are multiple\n",
    "        child nodes with the same max UCB, randomly select one\n",
    "        (using the UCB definition mentioned above with the exploration constant)\n",
    "    :param node: The parent node\n",
    "    :param exploration_const: The exploration constant in UCB formula\n",
    "    :return: an (action, child) tuple representing the best choice in terms of UCB,\n",
    "             where action is an Action object and child is a Node object\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    best_ucb = -math.inf\n",
    "    best_actions = []\n",
    "        \n",
    "    for action, child in node.children.items():\n",
    "        node_ucb = (child.tot_reward / child.num_samples\n",
    "                    + exploration_const *\n",
    "                    math.sqrt(2.0 * math.log(node.num_samples) /\n",
    "                              child.num_samples))\n",
    "        if node_ucb > best_ucb:\n",
    "            best_ucb = node_ucb\n",
    "            best_actions = [action]\n",
    "        elif node_ucb == best_ucb:\n",
    "            best_actions.append(action)\n",
    "        \n",
    "    best_action = random.choice(best_actions)\n",
    "    return best_action, node.children[best_action]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1728113dfcfcc33f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_select(select)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `expand` (10 points) <a id=\"expansion\"/>\n",
    "<img src=\"img/paper-expansion.png\" style=\"width: 700px;\" >\n",
    "Suppose that you have selected a non-terminal node that has not yet been expanded. As a reminder, a node “terminal” means that we stop simulation, assess the reward that was accumulated, and back-propagate the results up the tree. The terminal condition could be based on time, the depth of the tree, etc. Also, as a reminder, a node being “expanded” means that all of the possible actions that can be taken from the node are already in the tree as children of the node. The UCB selection process only considers fully expanded nodes. Once a node is reached that has not yet been “expanded”, the `expand` process is called. The `expand` process randomly selects an action from the list of available untaken actions, adds the new child node to the `children` dictionary, and returns the child node.\n",
    "\n",
    "Remember to used the [Provided Classes](#part-1-provided-classes) when implementing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cac40d673a2ac43",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def expand(node: Node) -> Node:\n",
    "    \"\"\" Randomly select an untried action and create a child node based on it\n",
    "        Return the new child node\n",
    "    :param node: The parent node\n",
    "    :return: The child node\n",
    "    \"\"\"\n",
    "    if node.is_expanded:\n",
    "        raise Exception(\"Should not expand a node that has already been expanded\")\n",
    "        \n",
    "    ### BEGIN SOLUTION\n",
    "    action = random.choice(node.unused_edges)\n",
    "    return node.add_child(action)\n",
    "    ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d6f102ff29b07b2b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_expand(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `default_rollout_policy` (10 points) <a id=\"rollout-policy\"/>\n",
    "<img src=\"img/paper-simulation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "As mentioned in [Part 0](#mcts-alg), the default rollout policy is to randomly select actions from the set of possible actions until a terminal state is reached.\n",
    "\n",
    "Remember to used the [Provided Classes](#part-1-provided-classes) when implementing!\n",
    "- `State` objects have a property `is_terminal` to represent whether a it is a terminal state\n",
    "- `State` objects have a property `possible_actions` which is a list of `Action` objects that are available from the current state.\n",
    "- `State` objects have a member  `execute_action(action)` which takes an `Action` object as input and returns a copy of the resulting `State` object after taking the action\n",
    "- `random.choice()` can be used to randomly select an action from a list of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04f2b4885acdea15",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def default_rollout_policy(state: State) -> float:\n",
    "    \"\"\" The default policy for simulation is to randomly (uniform distribution)\n",
    "        select an action to update the state and repeat the simulation until\n",
    "        a terminal state is reached\n",
    "    :param state: The starting state\n",
    "    :return: The reward at the terminal state\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while not state.is_terminal:\n",
    "        action = random.choice(state.possible_actions)\n",
    "        state = state.execute_action(action)\n",
    "    return state.reward\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c3db0bef45352e1",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_default_rollout_policy(default_rollout_policy)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `backpropagate` (10 points) <a id=\"backpropagate\"/>\n",
    "<img src=\"img/paper-back-propagation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "After simulation is performed and a terminal node is reached, the next thing to do is to update the visit counts and the total reward achieved for relevant nodes. In this case, the relevant nodes are sequence of parent node after parent node until the root node is reached. \n",
    "\n",
    "Remember to used the [Provided Classes](#part-1-provided-classes) when implementing!\n",
    "- `Node` objects have a member property `parent`\n",
    "- `Node` objects store visit count and total reward at member variables `num_samples` and `tot_reward` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20127172b9eb7705",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backpropagate(node: Node, reward: float = 0.0) -> None:\n",
    "    \"\"\" Propagate the reward and sample count from the specified node\n",
    "        back all the way to the root node (the node with None as parent)\n",
    "    :param node: The node wheresimulation is performed and\n",
    "        the reward is evaluated\n",
    "    :param reward: The reward at the node\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while node is not None:\n",
    "        node.num_samples += 1\n",
    "        node.tot_reward += reward\n",
    "        node = node.parent\n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e77f09e169ac4a3e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_backpropagate(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Written Question: *MCTS with Heuristics* (10 points) <a id=\"heuristics\"/>\n",
    "\n",
    "Now that you have implemented the MCTS algorithm, you should be ready to use it run it on the Gomoku problem. The Gomoku game that we provide you with has already been partially played out in order to prevent long run-times on the Jupyter hub. That said, please note that the simulation may take up to 3-5 minutes to complete the game. To ensure that the simulation always terminates in a reasonable number of steps and to make sure that the results observed by everyone are similar, we have fixed the random seed. Also, note that black tokens start off with an advantage, so if they were a good Gomoku player, they should always win. Keep this in mind when you interpret the simulation results. The initial game state is shown below.\n",
    "\n",
    "<img src=\"img/gomoku_example_initial_state.png\" style=\"width: 500px;\" >\n",
    "\n",
    "The following two code bocks offer two different simulations that you can test your MCTS algorithm on. The first simulation considers all free board spaces as a possible “next_action”. However, the second simulation only considers the free board spaces that are adjacent to a non-free board space to be part of the set of possible “next_actions”. Essentially, this adjustment serves as a heuristic that biases the search towards adjacent action sequences rather than playing moves randomly across the board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "simulate_with_black_sample_arbitrarily() missing 1 required positional argument: 'mcts_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-73244b501698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                        \u001b[0mexpand_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                        \u001b[0msimulation_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_rollout_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                        backpropagate_policy=backpropagate)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: simulate_with_black_sample_arbitrarily() missing 1 required positional argument: 'mcts_class'"
     ]
    }
   ],
   "source": [
    "simulate_with_black_sample_arbitrarily(select_policy=select,\n",
    "                                       expand_policy=expand,\n",
    "                                       simulation_policy=default_rollout_policy,\n",
    "                                       backpropagate_policy=backpropagate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "simulate_with_black_sample_neighborhood() missing 4 required positional arguments: 'select_policy', 'expand_policy', 'simulation_policy', and 'backpropagate_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7b55e017be8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate_with_black_sample_neighborhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonteCarloSearchTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: simulate_with_black_sample_neighborhood() missing 4 required positional arguments: 'select_policy', 'expand_policy', 'simulation_policy', and 'backpropagate_policy'"
     ]
    }
   ],
   "source": [
    "simulate_with_black_sample_neighborhood(MonteCarloSearchTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-793cd5e209540b17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** Which strategy generates a better result (the fully random strategy or the heuristic-augmented strategy)? Explain why. Could you come up with some potential methods to improve the performance of MCTS in Gomoku games?\n",
    "\n",
    "**Note:** if you had trouble getting the simulation to run, you can turn the block below that starts with *“## MCTS Team Results ”* from a Jupyter **code** block to a Jupyter **markdown** block, so you should still be able to answer the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6d22922079895a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## MCTS Team Results \n",
    "\n",
    "### Simulation 1: <img src=\"img/gomoku_example_simulation_result_black_takes_any_moves.png\" style=\"width: 500px;\" >\n",
    "\n",
    "### Simulation 2: <img src=\"img/gomoku_example_simulation_result_black_takes_neighboring_moves.png\" style=\"width: 500px;\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d4021b380fffb2a4",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: MCTS for Collaboration <a id=\"part-2\"/>\n",
    "<img src=\"img/auv-random-walk.png\" style=\"width: 400px;\" >\n",
    "\n",
    "\n",
    "In part 1, you have implemented the MCTS algorithm; in part 2, you will learn how to model a problem which can call that algorithm\n",
    "\n",
    "---\n",
    "## AUV Reward Collection Problem\n",
    "Suppose you are given 3 AUVs and your task is to use them to explore valuable locations of an underwater area, which has been spatially discretized below:\n",
    "\n",
    "<img src=\"img/maze_example_1.png\" style=\"width: 600px;\" >\n",
    "\n",
    "The time is also discretized into 15 steps. At each time step, an AUV can move to one of the 4 neighboring locations as long as it is not an obstacle (the red tiles in the figure). The radius of blue circles represent the information values of different locations: the larger radius corresponds to 3, and the smaller represents 1.\n",
    "\n",
    "We refer to this example as Maze Example 1. The task is to model this problem so the algorithm developed in Part 1 can be applied (and you will know how we implemented the Gomoku game because they are quite similar).\n",
    "\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## Provided Classes <a id=\"part-2-provided-classes\"/>\n",
    "\n",
    "We have provided a class `UnfinishedMazaState`, and your job is to finish building the class `MazeState`. To be more specific, you need to implement 4 key properties/methods, namely `is_terminal`, `reward`, `possible_actions` and `take_action`. You may find the following information useful.\n",
    "\n",
    "### `MazeAction` Class\n",
    "1. Can be constructed using `MazeAction(agent_index: int, position: (int, int)`, where `agent_index` indicates which AUV should move, and position is a tuple representing where it is moving to\n",
    "1. Has properties `agent_index` and `position`\n",
    "\n",
    "### `MazeEnvironment` Class\n",
    "1. Has property `rewards`, whcih is a dictionary mapping position (int, int) to reward value (float)\n",
    "2. Has property `obstacles`, which is a set of position (int, int)\n",
    "\n",
    "### `UnfinishedMazeState` Class\n",
    "1. Has a method `is_in_range` that takes (int, int) as input and returns a bool, indicating whether a position is inside the problem domain\n",
    "1. Has a property `paths` - \\[`path_0`, `path_1`, `path_2`, $\\ldots$\\]. Each `path_i` is a list of position tuples (int, int); the last tuple is current position of agent $i$. This records the trajectories of all agents\n",
    "1. Has a property `time_remains` which is an int object\n",
    "1. Has a property `visited` which is a set of visited position tuples (int, int)\n",
    "1. Has a property `turn` - an int object indicating which agent should move\n",
    "1. Has a property `switch_agent` which will properly update `turn` so the next agent can move; if all agents have moved for a round, then the `time_remains` property would be updated\n",
    "1. Has a property `environment`, which is a `MazeEnvironment` object\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each) <a id =\"maze\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(UnfinishedMazeState):\n",
    "\n",
    "    def __init__(self, environment: MazeEnvironment, time_remains: int = 10):\n",
    "        \"\"\" Create a state of the AUV reward-collection game\n",
    "        \"\"\"\n",
    "        super(UnfinishedMazeState, self).__init__(environment, time_remains)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def reward(self) -> float:\n",
    "        \"\"\" The total reward at the current state is value of\n",
    "            rewards that have been visited by agents\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        reward = 0.0\n",
    "        for target_position in self._environment.rewards:\n",
    "            if target_position in self.visited:\n",
    "                reward += self._environment.rewards[target_position]\n",
    "        return reward\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\" The only terminal condition is no time remains\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.time_remains <= 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "        \n",
    "    @property\n",
    "    def possible_actions(self) -> list:\n",
    "        \"\"\" The possible actions based on the current state\n",
    "            Note that you are only moving a single agent\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        i, j = self._paths[self._turn][-1]\n",
    "        actions = [MazeAction(self._turn, (i + 1, j)),\n",
    "                   MazeAction(self._turn, (i - 1, j)),\n",
    "                   MazeAction(self._turn, (i, j + 1)),\n",
    "                   MazeAction(self._turn, (i, j - 1))]\n",
    "        return [MazeAction(self._turn, action.position) for action in actions if\n",
    "                self.is_in_range(action.position) and\n",
    "                action.position not in self._environment.obstacles]\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    def take_action(self, action: MazeAction) -> \"MazeState\":\n",
    "        \"\"\" Execute the action based on the current state\n",
    "            You can assume that the action is always valid\n",
    "        :param action: The action\n",
    "        :return: The state after being updated\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self._paths[self._turn].append(action.position)\n",
    "        self._turn = (self._turn + 1) % len(self._paths)\n",
    "        if self._turn == 0:  # When all agents have taken a turn of actions\n",
    "            self._time_remains -= 1\n",
    "        return self\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your code is validated, you can run the simulation. Note that the simulation would take several minutes and we have fixed the random seed.\n",
    "\n",
    "You can check the check the correctness of your code in incremental steps \n",
    "TODO -- mention that tests assume order of implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_1() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-14ddfb3b1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_1() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_1()).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1315a5a1f55ef576",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_reward(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5a1a4a7e0465456",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_is_terminal(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-727963e36b39a45f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_possible_actions(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-53bbf722c18cb613",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_take_action(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Written Question: *Optimal Strategy* (5 points) <a id=\"written-optimal\"/>\n",
    "\n",
    "What is the optimal strategy? You only need to conceptually describe it. There is no need to enter the exact moves for each time steps.\n",
    "\n",
    "For example:\n",
    "> The AUV starting at (13, 8) should first collect the reward at (14, 10), (14, 11), and then move towards the reward at (15, 8)\n",
    "> The AUV starting at (7, 7) should move towards the reward at (11, 2)\n",
    "> The AUV starting at (12, 12) should move towards the reward at (4, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25cdfc53a4e11f2d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508fc0042443dd14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, run the simulation. The execution will again take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40f6bdd9ff826f63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_1() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-14ddfb3b1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_1() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_1()).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e39854823691bcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Written Question: *Maze Comparison* (5 points) <a id=\"written-comparison\"/>\n",
    "\n",
    "\n",
    "When using the default rollout policy, do you think MCTS will perform equally well in Maze Example 2 as in Maze Example 1? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ae33ce4e0c56bfb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_2() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-eef44224e8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_2() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_2()).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b07cc8047b2ed26e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af0a8bf29b28875b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## References <a id=\"references\">\n",
    "[1] Browne et. al. 2012, *A Survey of Monte Carlo Tree Search Methods*, IEEE.\n",
    "\n",
    "[2] Auer et. al. 2001, *Finite-time Analysis of the Multiarmed Bandit Problem*, International Conference on Machine Learning.\n",
    "\n",
    "[3] Silver et. al. 2017, *Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm*, arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
