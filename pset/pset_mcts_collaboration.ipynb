{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d807736505668a01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "import time\n",
    "import unittest\n",
    "\n",
    "from mcts  import *\n",
    "from state import AbstractState  as State, AbstractAction as Action\n",
    "from test  import *\n",
    "\n",
    "from gomoku_example import *\n",
    "from maze_unfinished import *\n",
    "from maze_example import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a00186f3414d78b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Mini-Pset: Monte Carlo Tree Search for Collaboration\n",
    "<img src=\"img/random-xkcd.png\"/>\n",
    "\n",
    "1. [Part 0: Background Information](#part-0)\n",
    "  1. [Structure of the Mini-Pset](#structure)\n",
    "  1. [*Monte Carlo* meets *Tree Search*](#monte-carlo-meets-tree-search)\n",
    "  1. [MCTS Algorithm](#mcts-alg)\n",
    "1. [Part 1: Introduction to MCTS](#part-1)\n",
    "  1. [Provided Classes](#part-1-provided-classes) \n",
    "  1. [Implement: `select` (10 points)](#selection)\n",
    "  1. [Implement: `expand` (10 points)](#expansion)\n",
    "  1. [Implement: `default_rollout_policy` (10 points)](#rollout-policy)\n",
    "  1. [Implement: `backpropagate` (10 points)](#backpropagate)\n",
    "  1. [Written Question: *MCTS with Heuristics* (10 points)](#heuristics)\n",
    "1. [Part 2: MCTS for Collaboration](#part-2)\n",
    "  1. [Provided Classes](#part-2-provided-classes) \n",
    "  1. [Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each)](#maze)\n",
    "  1. [Written Question: *Optimal Strategy* (5 points)](#written-optimal)\n",
    "  1. [Written Question: *MCTS Performance* (5 points)](#written-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-480319887bc49eb1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 0: Background Information <a id=\"part-0\"/>\n",
    "\n",
    "## Structure of the Mini-Pset <a id=\"structure\"/>\n",
    "This mini-pset is designed to give you an introduction to Monte Carlo Tree Search (MTCS) as a general planning technique as well as an introduction to using MCTS in a collaborative multi-agent scenario. The grading breakdown for this mini-pset can be seen in the outline above.\n",
    "\n",
    "**Part 0:** provides an overview of the mini-pset, along with helpful background information regarding MCTS. The MCTS algorithm itself is described in the [MCTS Algorithm](#mcts-alg) section.\n",
    "\n",
    "**Part 1:** focuses on introducing the core mechanics of MCTS: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*.  This is achieved by implementing each of these core methods and then exploring the results of MCTS when applied to [Gomoku](https://en.wikipedia.org/wiki/Gomoku).\n",
    "\n",
    "**Part 2:** focuses on introducing the key sub-routines that allow the MCTS functions from part 1 to be used in a collaborative multi-agent scenario. This is done by applying MCTS to a multi-agent value collection scenario that is inspired by the Kolumbo mission.\n",
    "\n",
    "Hint: the pset is meant to let you have fun with MCTS algorithm, not to be torturing. Therefore, if you find your code is very long (for example, over 20 lines for a single function), it is time to take a closer look at our hints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ebe9de1679d8ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## *Monte Carlo* meets *Tree Search* <a id=\"monte-carlo-meets-tree-search\"/>\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-pi-estimate.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"img/random-tree-search.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Generally speaking, *Monte Carlo* methods are a set of algorithms that use random sampling techniques to estimate the value of unknown parameters. Monte Carlo methods are particularly useful when the state space in question is large enough that brute force enumeration is infeasible. By employing the Law of Large numbers, we know that *“As the number of identically distributed, randomly generated variables increases, their sample mean approaches their theoretical mean.”* This law serves as the basis for Monte Carlo methods. As a simple example, Monte Carlo methods can be used to estimate the mathematical value $\\pi$. This is shown in the image above.\n",
    "\n",
    "Trees on the other hand are a restricted form of a graph that allows us to model a wide variety of decision making processes. Importantly, we can employ search methods over a tree to quickly find the optimal solution, given that the tree is of reasonable size. \n",
    "\n",
    "So what happens when we combine both paradigms? One way to think about *Monte Carlo Tree Search* is that the *Monte Carlo* mechanism serves as a method for estimating the value of a particular state in the game, done commonly with a random rollout policy, and the *Tree Search* mechanism uses an exploitation-exploration tradeoff with the value estimate and the visitation count of the node to determine which node to expand next. \n",
    "\n",
    "MCTS methods stand out in scenarios when the branching factor is quite high, there are no hand-constructed heuristics available, or an anytime solution is required.  \n",
    "\n",
    "### Bandit Problems\n",
    "<img src=\"img/random-bandit-cartoon.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Bandit Problems* are a class of sequential decision making problems where an agent must chose between $K$ actions in order to maximize the cumulative reward of the agent for some amount of time horizon.  For example, the agent (the bandit) in the scenario shown in the image above is an octopus, and the possible actions available to the octopus at any point in time correspond to the available slot machine levers. The goal of the octopus is to devise an optimal policy via experimentation because the underlying reward distribution associated with each lever is unknown. This means that the octopus must estimate the potential reward of pulling a particular lever based on the past observations of taking this action. This introduces the **exploitation-exploration dilemma**: the bandit needs to take advantage of the knowledge it has already gained about the system to make choices associated with high rewards, but the bandit must also must consider unexplored actions to learn more about the underlying reward distribution of said action. \n",
    "\n",
    "A useful construct for encoding the exploitation-exploration dilemma is **regret**, where regret is the expected loss for the bandit due to not acting optimally.  In this sense, an appropriate strategy for the bandit would be to minimize this regret. To express the regret of the bandit after *n* plays, defined as $R_n$ we can write: \n",
    "\n",
    "$$R_n = \\mu^*n - \\mu_j \\Sigma_{j=1}^{K} \\mathbb{E}[T_k(n)]$$\n",
    "\n",
    "where $\\mu^*$ is the best possible expected reward and $\\mathbb{E}[T_j(n)]$ is the expected number of executions of action $j$ across $n$ trials [[1]](#references). This leads us to the notion of an Upper Confidence Bound (UCB) that a particular action is optimal because UCB suggests a policy to follow in order to minimize the growth of regret for the bandit over time.\n",
    "\n",
    "### Upper Confidence Bound (UCB)\n",
    "A useful notion for tackling bandit problems is the upper confidence bound (UCB) that a particular action is the optimal action.  The UCB is useful because it allows the agent to consider both exploitation and exploration when considering what state to explore next. Perhaps the most famous example of a UCB is known as UCB1, shown below. \n",
    "\n",
    "$$\\text{UCB1} = \\bar{X_j} + \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$\n",
    "\n",
    "The first term, $\\bar{X_j}$  represents the current estimated value of action $j$, which improves over time as sampling continues. This serves the exploitation term: when the average value of a particular action is high, we would like to exploit this action in the future. The next term,  $\\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$ represents the number of times that node $j$ has been sampled relative to its siblings. Here, $n$ is the number of times that node $j$’s parent has been visited in the tree, and $n_j$ is the number of times that node $j$ has been visited. This serves as the exploration term: as the value of $n_j$ relative to $n$ decreases, the UCB value increases, and we are encouraged to explore relatively unexplored states. This version of UCB can readily be extended to include an additional weighting term between the exploration and exploitation terms. We won’t get into the math here, but to summarize, the UCB1 method takes advantage of the Chernoff-Hoeffding inequality to establish provably slow growth of regret for multi-bandit problems [[2]](#references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffd73cbbb6476b5a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## MCTS Algorithm <a id=\"mcts-alg\">\n",
    "    \n",
    "As discussed in lecture, the MCTS algorithm is a general framework that consists of four key phases: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*. During the *Selection* and *Expansion* phases, the *Tree Policy* is used, while during the *Simulation* phase, the *Default Policy* is used. The MCTS algorithm allows the *Tree Policy* and *Default Policy* to take on many forms. In this mini-pset, we will focus on using the Upper Confidence Bound for Trees (UCT) to serve as the *Tree Policy* and Random Rollout as the *Default Policy.* Both of these policies are discussed in more detail below.\n",
    "\n",
    "<img src=\"img/paper-mcts.png\" style=\"width: 700px;\" >\n",
    "\n",
    "Before we get into UCT and Random Rollout, it is important to remind ourselves of the two fundamental concepts that MCTS is build upon: the value of an action may be approximated by using random simulation, and that these values may be used efficiently to adjust the policy towards a best-first strategy. \n",
    "\n",
    "### Upper Confidence Bound for Trees (UCT)\n",
    "Generally speaking Upper Confidence Bound for Trees (UCT) refers to an MCTS algorithm that utilizes some UCB variant in the tree selection policy. In our case, we will use UCB1 with an added weighting factor as our selection policy. This means that during our selection phase, we select the node with the best UCT value. Keep in mind that the UCT term already includes both exploitation and exploration incentives. Pulling things together, we get the following equation for UCT:\n",
    "\n",
    "$$\\text{UCT} = \\bar{X_j} + C_p \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$ \n",
    "\n",
    "Again $\\bar{X_j}$ is the estimated value of taking action $j$, $n$ is how times the parent node has been visited, $n_j$ is the number of times action $j$ has been taken from this state, and $C_p$ is the additional weighting factor that can toggle the relative importance of the exploitation and exploration terms. If multiple nodes are found to have the same UCT value, the tie is broken by randomly selecting one of the nodes.\n",
    "\n",
    "The UCT criteria for *Selection* and *Expansion* is what allows for asymmetric tree growth, shown in the figure below. This asymmetric tree growth is why MCTS performs better compared to brute-force exhaustive search and pruning techniques for adversarial games, such as Alpha-Beta pruning, because MCTS will focus its search on promising areas of the tree.\n",
    "\n",
    "<img src=\"img/paper-tree-growth.png\" style=\"width: 400px;\" >\n",
    "\n",
    "### Default Policy: Random Rollout \n",
    "Once an action has been selected and expanded, it is now time to perform simulation in order to retrieve an estimate for how good (or bad) this particular state is. This is where the *Default Policy* is used to get from the current state to a terminal state. Once at the terminal state, we can assess the value of this simulated version of the game and use back-propagation to share this information up the tree. As mentioned previously, we harness the power of random sampling in our *Default Policy* by employing *Random Rollout*. Random Rollout simply chooses actions randomly from the set of possible actions at a given state until a terminal state is reached. It’s that simple. It is also possible to incorporate some heuristics to guide the behavior of the rollout, but we will not consider heuristics at this time. \n",
    "\n",
    "The figure below shows an example of a random rollout for a Tic-Tac-Toe game. Since the blue circle ends up winning the game, they improve the estimated value of the first move that was made. \n",
    "\n",
    "<img src=\"img/random-tree-rollout.png\" style=\"width: 300px;\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun Example: AlphaGo Zero\n",
    "Before jumping into the implementation work of this problem set, we quickly mention a recent example of using MCTS to devise the best Go-playing AI system to date: AlphaGo Zero, to help motivate the power of MCTS. AlphaGo Zero three core components: MCTS, Neural Networks, and Reinforcement Learning. The MCTS is used in AlphaGo Zero to determine which node to expand next in the tree, just like how we will utilize MCTS in this mini-pset. However, AlphaGo Zero also uses a Neural Network to assess the value of a state and Reinforcement Learning to update the weights of this Neural Network. Also, AlphaGo Zero had the luxury of playing against itself MANY times (approximately 19.6 million self-play games), on a high-powered machine. That said, MCTS is still the workhorse that pucks out the next move to make. [[3]](#references)\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-go-performance.gif\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/random-lee-sedol.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-744c8c9c22888a18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " ---\n",
    "# Part 1: Introduction to MCTS <a id=\"part-1\"/>\n",
    "\n",
    "In Part 1 of the mini-pset, you are asked to implement the four stages of the MCTS algorithm. After validating your code for each of these stages, you can see how your algorithm helps build a Gomoku AI!\n",
    "\n",
    "The core objective of Gomoku is to get five of your tokens aligned in a row, either horizontally, vertically, or in either diagonal direction. At any given turn during the game, you are allowed to place your token in any free space. The game ends as soon as five-in-a-row is achieved somewhere on the board, and the game is typically played on a 15$\\times$15 board. In theory, if the black side, who moves first, always adopts the optimal strategy, it will always win. As a result, the actual rules in a professional-level Gomoku game is more complicated than this. However, the specific rules regarding how to play Gomoku are not important for the mini-pset because the core mechanics of MCTS are not specific to a particular game or scenario, and we are restricting ourselves to a 9$\\times$9 board. To support the simulation of the Gomoku game, we have provided you with `GomokuState` and `GomokuAction` classes that embody the mechanics of the Gomoku game. You will not need to interact with these classes directly because we have well-prepared wrappers for you to call. An example of the Gomoku game is shown below, and your job is to help the black side to win.\n",
    "\n",
    "<img src=\"img/gomoku_example_initial_state.png\" style=\"width: 400px;\" >\n",
    "    \n",
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## Provided Classes <a id=\"part-1-provided-classes\"/>\n",
    "\n",
    "\n",
    "You are now asked to finish several key components in Monte Carlo Tree Search, namely the `select`, `expand`, `default_rollout_policy` and `backpropagate` functions. You will need to use the `Node` class that we have provide.\n",
    "\n",
    "### `Node` Class\n",
    "\n",
    "`Node` **member variables** (accessible and modifiable via `my_node.children`, etc):\n",
    "1. `children`: a dictionary that maps `Action` objects to `Node` objects, representing the children of the  node\n",
    "1. `tot_reward`: a float of reward that has been back-propagated to this node\n",
    "1. `num_samples`: an int of number of samples that has been back-propagated to this node\n",
    "\n",
    "`Node` **properties** (accessible but not modifiable via `my_node.state`, etc):\n",
    "1. `state`: a `State` object corresponding to state of the node\n",
    "1. `parent`: a `Node` object that is the parent\n",
    "1. `unused_edges`: a list of `Action` objects that have not contributed to building child nodes (i.e. unexplored actions from this node)\n",
    "1. `is_terminal`: a bool representing whether or not the `State` object associated with the node is a terminal state\n",
    "1. `depth`: an int of the depth of the node. The depth of a root node is 1 (the parent of the root node is `None`)\n",
    "1. `is_expanded`: a bool representing whether all possible actions and associated child nodes have been added to the `children` dictionary\n",
    "\n",
    "`Node` **methods** (accessible via `my_node.add_child(new_action)`)\n",
    "1. `add_child(action)`: if `action` is in the list `unused_edges`, then this method builds a new `Node` object called `child`, adds the key-value pair `action`:`child` to the `children` dictionary, and returns returns `child`\n",
    "\n",
    "---\n",
    "**Note:** that in Python, the notation `def my_function() -> float:` indicates that the return value of `my_function` must be of type `float`. This is helpful for catching bugs that can arise from object type mis-matching.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Hints\n",
    "- when making random selection from a list of variables, use the `random.choice()` function\n",
    "- when evalauting the natural log, you can use `math.log()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1c1dc82cdbe3ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `select` (10 points) <a id=\"selection\"/>\n",
    "<img src=\"img/paper-selection.png\" style=\"width: 700px;\" >\n",
    "\n",
    "The standard way to select a node to expand is based on choosing the node with the maximal upper confidence bound (UCB):\n",
    "$$UCB=\\left(\\frac{\\text{total_reward}}{\\text{number_of_samples_of_the_child_node}}\\right)+\\sqrt{2\\frac{\\ln({\\text{number_of_samples_of_the_parent_node})}}{\\text{number_of_samples_of_the child_node}}}$$\n",
    "where the first term is the average reward per sample (the exploitation term), and the second term, coming from Hoeffding's inequality, will favor less unexplored choices (the exploration term).\n",
    "\n",
    "Later when we are choosing the best strategy for the agent (we are actually taking an action in real-life rather than simulating actions in MCTS), we pursue pure exploitation. This means that we need a way to toggle on and off the exploration term. To achieve this, we add an additional term called the “exploration_constant” to the equation, giving us the resulting definition of UCB:\n",
    "\n",
    "$$UCB=\\left(\\frac{\\text{total_reward}}{\\text{number_of_samples_of_the_child_node}}\\right)+\\text{exploration_constant}*\\sqrt{2\\frac{\\ln({\\text{number_of_samples_of_the_parent_node})}}{\\text{number_of_samples_of_the child_node}}}$$\n",
    "\n",
    "Remember to use our [Provided Classes](#part-1-provided-classes) when implementing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5146225d8db989b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select(node: Node, exploration_const: float = 1.0) -> (Action, Node):\n",
    "    \"\"\" Select the best child node based on UCB; if there are multiple\n",
    "        child nodes with the same max UCB, randomly select one\n",
    "        (using the UCB definition mentioned above with the exploration constant)\n",
    "    :param node: The parent node\n",
    "    :param exploration_const: The exploration constant in UCB formula\n",
    "    :return: an (action, child) tuple representing the best choice in terms of UCB,\n",
    "             where action is an Action object and child is a Node object\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    best_ucb = -math.inf\n",
    "    best_actions = []\n",
    "        \n",
    "    for action, child in node.children.items():\n",
    "        \n",
    "        node_ucb = (child.tot_reward / child.num_samples\n",
    "                    + exploration_const *\n",
    "                    math.sqrt(2.0 * math.log(node.num_samples) /\n",
    "                              child.num_samples))\n",
    "        if node_ucb > best_ucb:\n",
    "            best_ucb = node_ucb\n",
    "            best_actions = [action]\n",
    "        elif node_ucb == best_ucb:\n",
    "            best_actions.append(action)\n",
    "        \n",
    "    best_action = random.choice(best_actions)\n",
    "    return best_action, node.children[best_action]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1728113dfcfcc33f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_select(select)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efeacfea899cb758",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `expand` (10 points) <a id=\"expansion\"/>\n",
    "<img src=\"img/paper-expansion.png\" style=\"width: 700px;\" >\n",
    "Suppose that you have selected a non-terminal node that has not yet been expanded. As a reminder, a node “terminal” means that we stop simulation, assess the reward that was accumulated, and backpropagate the results along the tree. The `expand` process randomly selects an action from the list of available untaken actions, adds the new child node to the `children` dictionary, and returns the child node.\n",
    "\n",
    "(As a reminder, a node being “expanded” means that all of the possible actions that can be taken from the node are already in the tree as children of the node. The UCB selection process only considers fully expanded nodes. Once a node is reached that has not yet been “expanded”, the `expand` process is called) \n",
    "\n",
    "Remember to use the [Provided Classes](#part-1-provided-classes) when implementing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cac40d673a2ac43",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def expand(node: Node) -> Node:\n",
    "    \"\"\" Randomly select an untried action and create a child node based on it\n",
    "        Return the new child node\n",
    "    :param node: The parent node\n",
    "    :return: The child node\n",
    "    \"\"\"\n",
    "    if node.is_expanded:\n",
    "        raise Exception(\"Should not expand a node that has already been expanded\")\n",
    "        \n",
    "    ### BEGIN SOLUTION\n",
    "    action = random.choice(node.unused_edges)\n",
    "    return node.add_child(action)\n",
    "    ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d6f102ff29b07b2b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_expand(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b4e11919a6a3597",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `default_rollout_policy` (10 points) <a id=\"rollout-policy\"/>\n",
    "<img src=\"img/paper-simulation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "As mentioned in [Part 0](#mcts-alg), the default rollout policy is to randomly select actions from the set of possible actions until a terminal state is reached.\n",
    "\n",
    "Remember to used the [Provided Classes](#part-1-provided-classes) when implementing!\n",
    "- `State` objects have a property `is_terminal` to represent whether a it is a terminal state\n",
    "- `State` objects have a property `possible_actions` which is a list of `Action` objects that are available from the current state.\n",
    "- `State` objects have a method  `execute_action(action)` which takes an `Action` object as input and returns a copy of the resulting `State` object after taking the action\n",
    "- `random.choice()` can be used to randomly select an action from a list of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04f2b4885acdea15",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def default_rollout_policy(state: State) -> float:\n",
    "    \"\"\" The default policy for simulation is to randomly (uniform distribution)\n",
    "        select an action to update the state and repeat the simulation until\n",
    "        a terminal state is reached\n",
    "    :param state: The starting state\n",
    "    :return: The reward at the terminal state\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while not state.is_terminal:\n",
    "        action = random.choice(state.possible_actions)\n",
    "        state = state.execute_action(action)\n",
    "    return state.reward\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c3db0bef45352e1",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_default_rollout_policy(default_rollout_policy)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e6af4305649c938",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `backpropagate` (10 points) <a id=\"backpropagate\"/>\n",
    "<img src=\"img/paper-back-propagation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "After simulation is performed and a terminal node is reached, the next thing to do is to update the visit counts and the total reward achieved for relevant nodes. In this case, the relevant nodes are sequence of parent node after parent node until the root node is reached. \n",
    "\n",
    "Remember to used the [Provided Classes](#part-1-provided-classes) when implementing!\n",
    "- `Node` objects have a member property `parent`\n",
    "- `Node` objects store visit count and total reward at public variables `num_samples` and `tot_reward` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20127172b9eb7705",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backpropagate(node: Node, reward: float = 0.0) -> None:\n",
    "    \"\"\" Propagate the reward and sample count from the specified node\n",
    "        back all the way to the root node (the node with None as parent)\n",
    "    :param node: The node where simulation starts\n",
    "    :param reward: The reward at the terminal state of the simulation\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while node is not None:\n",
    "        node.num_samples += 1\n",
    "        node.tot_reward += reward\n",
    "        node = node.parent\n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e77f09e169ac4a3e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_backpropagate(backpropagate)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e10730d0e434e4c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Written Question: *MCTS with Heuristics* (10 points) <a id=\"heuristics\"/>\n",
    "\n",
    "Now that you have implemented the MCTS algorithm, you should be ready to use it run it on the Gomoku problem. The Gomoku game that we provide you with has already been partially played out in order to prevent long run-times on the Jupyter hub. That said, please note that the simulation may take up to 3-5 minutes to complete the game. To ensure that the simulation always terminates in a reasonable number of steps and to make sure that the results observed by everyone are similar, we have fixed the random seed. Also, note that black tokens start off with an advantage, so if they were a good Gomoku player, they should always win. Keep this in mind when you interpret the simulation results. The initial game state is shown below. Note that the numbers on the Gomoku board indicate the order of which the pieces were placed. In this example, the first move is the black token selecting position *(4,F)*, the second move is the white token selecting position *(4,G)*, and so on. \n",
    "\n",
    "<img src=\"img/gomoku_example_initial_state.png\" style=\"width: 500px;\" >\n",
    "\n",
    "The following two code bocks offer two different simulations that you can test your MCTS algorithm on. The first simulation considers all free board spaces as a possible “next_action”. However, the second simulation only considers the free board spaces that are adjacent to a non-free board space to be part of the set of possible “next_actions”. Essentially, this adjustment serves as a heuristic that biases the search towards adjacent action sequences rather than playing moves randomly across the board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30adc9ddbcd9c330",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulate_with_black_sample_arbitrarily(select_policy=select,\n",
    "                                       expand_policy=expand,\n",
    "                                       simulate_policy=default_rollout_policy,\n",
    "                                       backpropagate_policy=backpropagate,\n",
    "                                       num_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffeeeb639175e2ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "simulate_with_black_sample_neighborhood(select_policy=select,\n",
    "                                        expand_policy=expand,\n",
    "                                        simulate_policy=default_rollout_policy,\n",
    "                                        backpropagate_policy=backpropagate,\n",
    "                                        num_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Since the simulation is very random when the number of iterations is limited to a smaller value, and it takes quite long to perform a large-sample simulation, we provide everyone with sample results to answer the following question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6d22922079895a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## MCTS Team Results for the Gomoku Problem\n",
    " \n",
    "### Simulation 1: Black takes any free space\n",
    "<img src=\"img/gomoku_example_simulation_result_black_takes_any_moves.png\" style=\"width: 500px;\" >\n",
    "\n",
    "### Simulation 2:  Black only takes space with occupied neighbors<img src=\"img/gomoku_example_simulation_result_black_takes_neighboring_moves.png\" style=\"width: 500px;\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-793cd5e209540b17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** Based on the sample results, which strategy generates a better result (the fully random strategy or the heuristic-augmented strategy)? Explain why. Probably you are still not satisfied with your AI, especially when the number of samples is limited. Could you come up with some potential methods to improve the performance of MCTS in Gomoku games?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d4021b380fffb2a4",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee61f3846aafc322",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Part 2: MCTS for Collaboration <a id=\"part-2\"/>\n",
    "\n",
    "In Part 1, you have implemented the core MCTS algorithm. In Part 2, you will learn the usage of the MCTS functions that you have already implemented, and how to model a situation that supports multi-agent collaboration. In fact, to apply the MCTS algorithm in Part 1 to any problem, all you need to do is to create a class with member properties `reward`, `is_terminal`, `possible_actions` and member method `execute_action`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83e5a0a7a1657c49",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## AUV Reward Collection Problem\n",
    "Suppose you are given 3 AUVs and your task is to use them to explore valuable locations of an underwater area, which has been spatially discretized into a 2-D grid system. An example of this scenario is shown below:\n",
    "\n",
    "<img src=\"img/maze_example_1.png\" style=\"width: 500px;\" >\n",
    "\n",
    "The time-horizon is also discretized into 15 steps. Vehicle dynamics are ignored and the possibility of vehicle-vehicle collisions are ignored. At each time step, an AUV can move to one of the four adjacent tiles in the NSEW cardinal directions (no diagonal transitions are allowed). However, vehicles are not allowed to transition into a tile that contains an obstacle. Obstacles are the red tiles in the visualization. The valued regions are shown as blue circles, with the larger circles representing reward of 3.0 and smaller circles being rewards of 1.0. \n",
    "\n",
    "We refer to example above as “Maze Example 1”. The task is to model this problem so that the MCTS functions implemented in Part 1 can be applied; and you will know how we implemented the Gomoku game because they are quite similar (if not but you are interested, you can find the implementation in *gomoku.py*).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83e404097a1657c49",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## Provided Classes <a id=\"part-2-provided-classes\"/>\n",
    "\n",
    "We have provided a class `UnfinishedMazeState`, and your job is to finish building the class `MazeState`, which is a subclass of `UnfinishedMazeState` (this means that `MazeState` inherits many of the AUV-specific functions from `UnfinishedMazeState`, but not all of the functions necessary for compatibility with MCTS are implemented yet — your job is to implement these functions) . To be more specific, you need to implement four key properties/methods, namely `is_terminal`, `reward`, `possible_actions` and `execute_action`. We have have provided you with the following classes:\n",
    "\n",
    "### `MazeAction` Class\n",
    "`MazeAction` Constructor:\n",
    "1. `MazeAction(agent_index, position)`: constructs a `MazeAction` object,  where `agent_index` is an int that indicates which AUV should move, and position is an `(int, int)` tuple representing the location the agent is moving toward. \n",
    "    - For example `MazeAction(0, (10,10))` would be a `MazeAction` object that represents agent 0 moving to position (10,10). \n",
    "\n",
    "`MazeAction` Properties: \n",
    "1.  `agent_index` : the index that represents which agent performed this action \n",
    "1.  `position`: the position that the agent moved to by taking this action\n",
    "\n",
    "---\n",
    "### `MazeEnvironment` Class\n",
    "`MazeEnvironment`  Properties:\n",
    "1. `rewards`: a dictionary mapping position of type `(int, int)` to a reward value of type `float`\n",
    "1. `obstacles`: a set of position locations `(int, int)` , such that every grid cell contained in the `obstacles` set is considered an obstacle\n",
    "\n",
    "---\n",
    "### `UnfinishedMazeState` Class\n",
    "`UnfinishedMazeState` Properties:\n",
    "1. `paths`: a list of paths, \\[`path_0`, `path_1`, `path_2`, $\\ldots$\\], where each path `path_i` is a list of position tuples `(int, int)` that represent the trajectory of agent `i`. The first `(int,int)` tuple in `path_i` is the initial position of agent `i` and the last `(int, int)` tuple in `path_i` is the current position of agent `i`. pa the last tuple is current position of agent $i$. This property records the trajectories of all agents. \n",
    "1. `time_remains`: an int that represents the number of time steps remaining in the problem (this represents the time-horizon constraint that the mission must be completed by *x* time)\n",
    "1. `visited`: the set of visited position tuples `(int, int)` (the union of the visit locations amongst the multiple agents)\n",
    "1. `turn`: an int object indicating which agent should move next. It is assumed the first agent is given by `int=0`\n",
    "1. `environment`: a `MazeEnvironment` object that represents the environment\n",
    "\n",
    "`UnfinishedMazeState` Methods:\n",
    "1. `switch_agent()`:  updates the `turn` property to indicate that it is next agents turn. Once all agents have moved for a given time-interval round, then the `time_remains` property would be updated.\n",
    "1. `is_in_range(position)`: takes in a position tuple `(int, int)` as input and returns a bool, that indicates  whether a position is inside the problem domain.\n",
    "1. `__copy__()`: returns a copy of the current state. `deepcopy` is applied to necessary member variables.\n",
    "\n",
    "---\n",
    "**Note:** that in Python, the notation `def my_function() -> float:` indicates that the return value of `my_function` must be of type `float`. This is helpful for catching bugs that can arise from object type mis-matching.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd41ae2aab0ff26b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implement: `reward`, `is_terminal`, `possible_actions`, `execute_action` (10 points each) <a id =\"maze\">\n",
    "    \n",
    "Using the [Provided Classes](#part-2-provided-classes) above, implement the following properties and method:\n",
    "- `reward`: returns a `float` that represents the total collected reward given the state of the environment. Note that reward is only collected from a location once the location has been visited, and multiple rewards can not be retrieved from the same position.\n",
    "\n",
    "\n",
    "- `is_terminal`: returns a bool if and only if the the state is terminal. A state is terminal if and only if no time remains.\n",
    "\n",
    "\n",
    "- `possible_actions`: returns the list of `MazeAction` objects that represent the possible actions that can be taken from this state of the `MazeState`. Note that you are only generating the possible actions for a single agent. To do so, you must consider the following: what agent must take an action next (see [Provided Classes](#part-2-provided-classes) if you do not know), where this agent is currently located, the viable domain of the problem, and the obstacles in the environment.\n",
    "\n",
    "\n",
    "- `execute_action(maze_action)`: given a `MazeAction` action, executes the action in a separate copy (because when you are doing MCTS, your current state does not really change, so you want to perform simulation on a copy) of the current state and returns that copy after completing the action.  To do so, you must consider the following: the effect of executing this action on the `paths` property of `MazeState`, the effect of executing this action on the `turn` property of `MazeState` (remember that we have already provided a good method to update it), and the effect of executing this action on the `time_remains` property of `MazeState`. \n",
    "    \n",
    "*Note that you can implement these methods sequentially because the test functions only require that the individual functions are correct, not that the entire class is implemented correctly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b0bdf7561c6381e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MazeState(UnfinishedMazeState):\n",
    "\n",
    "    def __init__(self, environment: MazeEnvironment, time_remains: int = 10):\n",
    "        \"\"\" Create a state of the AUV reward-collection game\n",
    "        \"\"\"\n",
    "        super().__init__(environment, time_remains)\n",
    "\n",
    "    @property\n",
    "    def reward(self) -> float:\n",
    "        \"\"\" The total reward at the current state is value of\n",
    "            rewards that have been visited by agents\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        reward = 0.0\n",
    "        for target_position in self.environment.rewards:\n",
    "            if target_position in self.visited:\n",
    "                reward += self.environment.rewards[target_position]\n",
    "        return reward\n",
    "        ### END SOLUTION\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\" The only terminal condition is no time remains\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.time_remains <= 0\n",
    "        ### END SOLUTION\n",
    "\n",
    "    @property\n",
    "    def possible_actions(self) -> list:\n",
    "        \"\"\" The possible actions based on the current state\n",
    "            Note that you are only moving a single agent\n",
    "            In our Provided Classes section, you can find\n",
    "            which agent you are moving\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        i, j = self._paths[self.turn][-1]\n",
    "        actions = [MazeAction(self.turn, (i + 1, j)),\n",
    "                   MazeAction(self.turn, (i - 1, j)),\n",
    "                   MazeAction(self.turn, (i, j + 1)),\n",
    "                   MazeAction(self.turn, (i, j - 1))]\n",
    "        return [action for action in actions if\n",
    "                self.is_in_range(action.position) and\n",
    "                action.position not in self.environment.obstacles]\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def execute_action(self, action: MazeAction) -> \"MazeState\":\n",
    "        \"\"\" Execute the action based on the current state\n",
    "            You can assume that the action is always valid\n",
    "            Be sure to update the agent and time using the information\n",
    "            we provided in the Provided Class section\n",
    "        :param action: The action\n",
    "        :return: The state after being updated\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        new_state = self.__copy__()\n",
    "        new_state.paths[action.agent_index].append(action.position)\n",
    "        new_state.switch_agent()\n",
    "        return new_state\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1315a5a1f55ef576",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_reward(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5a1a4a7e0465456",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_is_terminal(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-727963e36b39a45f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_possible_actions(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-53bbf722c18cb613",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_take_action(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed208f2fad599bca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After your code is validated, you can run the simulation. Note that the simulation would take several minutes and we have fixed the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40f6bdd9ff826f63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulate(MonteCarloSearchTree, initial_state=maze_example_1(MazeState),\n",
    "         mcts_select_policy=select,\n",
    "         mcts_expand_policy=expand,\n",
    "         mcts_rollout_policy=default_rollout_policy,\n",
    "         mcts_backpropagate_policy=backpropagate).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44ca767688d476b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Written Question: *Optimal Strategy* (5 points) <a id=\"written-optimal\"/>\n",
    "\n",
    "Now consider a different problem, which we refer to as Maze Example 2, as shown below. In this figure, the largest blue circles have rewards of 10.0, the smallest have 1.0 and the intermediate have 4.0.\n",
    "\n",
    "<img src=\"img/maze_example_2.png\" style=\"width: 500px;\" >\n",
    "\n",
    "**Question:** What is the optimal strategy, given the reward structure and time-horizon constraints mentioned above? You only need to conceptually describe it. There is no need to enter the exact moves for each time steps.\n",
    "\n",
    "For example:\n",
    "> The AUV starting at (13, 8) should first collect the reward at (14, 10), (14, 11), and then move towards the reward at (15, 8)\n",
    "> The AUV starting at (7, 7) should move towards the reward at (11, 2)\n",
    "> The AUV starting at (12, 12) should move towards the reward at (4, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25cdfc53a4e11f2d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3dkfhkdn4823691bcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Written Question: *MCTS Performance* (5 points) <a id=\"written-performance\"/>\n",
    "\n",
    "Run the simulation for Maze Example 2. The execution will again take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ae33ce4e0c56bfb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulate(MonteCarloSearchTree, initial_state=maze_example_2(MazeState),\n",
    "         mcts_select_policy=select,\n",
    "         mcts_expand_policy=expand,\n",
    "         mcts_rollout_policy=default_rollout_policy,\n",
    "         mcts_backpropagate_policy=backpropagate).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e39854823691bcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question:** Does MCTS find the optimal strategy? When using the default rollout policy, do you think MCTS has the ability to find the optimal strategy in Maze Example 2? Explain why.\n",
    "\n",
    "**Note:** if you had trouble getting the simulation to run, you can refer to the plots shown below under *“Sample Results”*, so you should still be able to answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b07cc8047b2ed26e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79c5f445b22a3a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Sample Results for the AUV Value Collection Problem\n",
    " \n",
    "### Maze Example 1: <img src=\"img/maze_example_1_result.png\" style=\"width: 500px;\" >\n",
    "\n",
    "### Maze Example 2: <img src=\"img/maze_example_2_result.png\" style=\"width: 500px;\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e812fcffe6796699",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Congratulations! You have finished a valuable pset where you have learnt how to build a modularized MCTS algorithm, and have practiced how to use it to solve problems!\n",
    "\n",
    "The MCTS code you have built today is more modularized than the `mcts` library you can install from pip. So do not discard it, and you may find it useful when you want to combine other algorithms with MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af0a8bf29b28875b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## References <a id=\"references\">\n",
    "[1] Browne et. al. 2012, *A Survey of Monte Carlo Tree Search Methods*, IEEE.\n",
    "\n",
    "[2] Auer et. al. 2001, *Finite-time Analysis of the Multiarmed Bandit Problem*, International Conference on Machine Learning.\n",
    "\n",
    "[3] Silver et. al. 2017, *Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm*, arXiv."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
