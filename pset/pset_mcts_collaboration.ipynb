{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d807736505668a01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-33bffe6e56a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractState\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAbstractAction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmcts\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtest\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pucan/GitHub Repos/mcts/pset/mcts.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m def execute_round(root: Node, max_tree_depth: int = 15,\n\u001b[0;32m---> 89\u001b[0;31m                   \u001b[0mtree_select_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_expand_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                   \u001b[0mrollout_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_rollout_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                   backpropagate_method=backpropagate) -> None:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'select' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "import time\n",
    "import unittest\n",
    "\n",
    "from state import AbstractState as State, AbstractAction as Action\n",
    "from mcts  import *\n",
    "from test  import *\n",
    "\n",
    "from gomoku_example import *\n",
    "from maze_unfinished import *\n",
    "from maze_example    import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a00186f3414d78b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Mini-Pset: Monte Carlo Tree Search for Collaboration\n",
    "<img src=\"img/random-xkcd.png\"/>\n",
    "\n",
    "1. [Part 0: Background Information](#part-0)\n",
    "  1. [Structure of the Mini-Pset](#structure)\n",
    "  1. [*Monte Carlo* meets *Tree Search*](#monte-carlo-meets-tree-search)\n",
    "  1. [MCTS Algorithm](#mcts-alg)\n",
    "1. [Part 1: Introduction to MCTS](#part-1)\n",
    "  1. [Provided Classes](#part-1-provided-classes) \n",
    "  1. [Implement: `select` (10 points)](#selection)\n",
    "  1. [Implement: `expand` (10 points)](#expansion)\n",
    "  1. [Implement: `default_rollout_policy` (10 points)](#rollout-policy)\n",
    "  1. [Implement: `backpropagate` (10 points)](#backpropagate)\n",
    "  1. [Written Question: *MCTS with Heuristics* (10 points)](#heuristics)\n",
    "1. [Part 2: MCTS for Collaboration](#part-2)\n",
    "  1. [Provided Classes](#part-2-provided-classes) \n",
    "  1. [Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each)](#maze)\n",
    "  1. [Written Question: *Optimal Strategy* (5 points)](#written-optimal)\n",
    "  1. [Written Question: *Maze Comparison* (5 points)](#written-comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffd73cbbb6476b5a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 0: Background Information <a id=\"part-0\"/>\n",
    "\n",
    "## Structure of the Mini-Pset <a id=\"structure\"/>\n",
    "This mini-pset is designed to give you an introduction to Monte Carlo Tree Search (MTCS) as a general planning technique as well as an introduction to using MCTS in a collaborative multi-agent scenario. The grading breakdown for this mini-pset can be seen in the outline above.\n",
    "\n",
    "**Part 0:** provides an overview of the mini-pset, along with helpful background information regarding MCTS. The MCTS algorithm itself is described in the [MCTS Algorithm](#mcts-alg) section. Provided code for completing Part 1 and Part 2 can be found in [part 1](#part-1-provided-classes) and [part 2](#part-2-provided-classes), respectively.\n",
    "\n",
    "**Part 1:** focuses on introducing the core mechanics of MCTS: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*.  This is achieved by implementing each of these core methods and then exploring the results of MCTS when applied to [Gomoku](https://en.wikipedia.org/wiki/Gomoku).\n",
    "\n",
    "**Part 2:** focuses on introducing the key sub-routines that allow the MCTS functions from part 1 to be used in a collaborative multi-agent scenario. This is done by applying MCTS to a multi-agent value collection scenario that is inspired by the Kolumbo mission. \n",
    "\n",
    "---\n",
    "## *Monte Carlo* meets *Tree Search* <a id=\"monte-carlo-meets-tree-search\"/>\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-pi-estimate.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"img/random-tree-search.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Generally speaking, *Monte Carlo* methods are a set of algorithms that use random sampling techniques to estimate the value of unknown parameters. Monte Carlo methods are particularly useful when the state space in question is large enough that brute force enumeration is infeasible. By employing the Law of Large numbers, we know that *“As the number of identically distributed, randomly generated variables increases, their sample mean approaches their theoretical mean.”* This law serves as the basis for Monte Carlo methods. As a simple example, Monte Carlo methods can be used to estimate the mathematical value Pi. This is shown in the image above.\n",
    "\n",
    "Trees on the other hand are a restricted form of a graph that allows us to model a wide variety of decision making processes. Importantly, we can employ search methods over a tree to quickly find the optimal solution, given that the tree is of reasonable size. \n",
    "\n",
    "So what happens when we combine both paradigms? One way to think about *Monte Carlo Tree Search* is that the *Monte Carlo* mechanism serves as a method for estimating the value of a particular state in the game, done commonly with a random rollout policy, and the *Tree Search* mechanism uses an exploitation-exploration tradeoff with the value estimate and the visitation count of the node to determine which node to expand next. \n",
    "\n",
    "MCTS methods perform particularly well in scenarios when the branching factor is quite high, there are no hand-constructed heuristics available, or an anytime solution is required.  \n",
    "\n",
    "### Bandit Problems\n",
    "<img src=\"img/random-bandit-cartoon.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "*Bandit Problems* are a class of sequential decision making problems where an agent must chose between $K$ actions in order to maximize the cumulative reward of the agent for some amount of time horizon.  For example, the agent (the bandit) in the scenario shown in the image above is an octopus, and the possible actions available to the octopus at any point in time correspond to the available slot machine levers. The goal of the octopus is to devise an optimal policy via experimentation because the underlying reward distribution associated with each lever is unknown. This means that the octopus must estimate the potential reward of pulling a particular lever based on the past observations of taking this action. This introduces the **exploitation-exploration dilemma**: the bandit needs to take advantage of the knowledge it has already gained about the system to make choices associated with high rewards, but the bandit must also must consider unexplored actions to learn more about the underlying reward distribution of said action. \n",
    "\n",
    "A useful construct for encoding the exploitation-exploration dilemma is **regret**, where regret is the expected loss for the bandit due to not acting optimally.  In this sense, an appropriate strategy for the bandit would be to minimize this regret. To express the regret of the bandit after *n* plays, defined as $R_n$ we can write: \n",
    "\n",
    "$$R_n = \\mu^*n - \\mu_j \\Sigma_{j=1}^{K} \\mathbb{E}[T_k(n)]$$\n",
    "\n",
    "where $\\mu^*$ is the best possible expected reward and $\\mathbb{E}[T_j(n)]$ is the expected number of executions of action $j$ across $n$ trials [1]. This leads us to the notion of an Upper Confidence Bound (UCB) that a particular action is optimal because UCB suggests a policy to follow in order to minimize the growth of regret for the bandit over time.\n",
    "\n",
    "### Upper Confidence Bound (UCB)\n",
    "A useful notion for tackling bandit problems is the upper confidence bound (UCB) that a particular action is the optimal action.  The UCB is useful because it allows the agent to consider both exploitation and exploration when considering what state to explore next. Perhaps the most famous example of a UCB is known as UCB1, shown below. \n",
    "\n",
    "$$UCB1 = \\bar{X_j} + \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$\n",
    "\n",
    "The first term, $\\bar{X_j}$  represents the current estimated value of action $j$, which improves over time as sampling continues. This serves the exploitation term: when the average value of a particular action is high, we would like to exploit this action in the future. The next term,  $\\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$ represents the number of times that node $j$ has been sampled relative to its siblings. Here, $n$ is the number of times that node $j$’s parent has been visited in the tree, and $n_j$ is the number of times that node $j$ has been visited. This serves as the exploration term: as the value of $n_j$ relative to $n$ decreases, the UCB value increases, and we are encouraged to explore relatively unexplored states. This version of UCB can readily be extended to include an additional weighting term between the exploration and exploitation terms. We won’t get into the math here, but to summarize, the UCB1 method takes advantage of the Chernoff-Hoeffding inequality to establish provably slow growth of regret for multi-bandit problems [2].\n",
    "\n",
    "\n",
    "---\n",
    "## MCTS Algorithm <a id=\"mcts-alg\">\n",
    "    \n",
    "As discussed in lecture, the MCTS algorithm is a general framework that consists of four key phases: *Selection*, *Expansion*, *Simulation*, and *Back-Propagation*. During the *Selection* and *Expansion* phases, the *Tree Policy* is used, while during the *Simulation* phase, the *Default Policy* is used. The MCTS algorithm allows the *Tree Policy* and *Default Policy* to take on many forms. In this mini-pset, we will focus on using the Upper Confidence Bound for Trees (UCT) to serve as the *Tree Policy* and Random Rollout as the *Default Policy.* Both of these policies are discussed in more detail below.\n",
    "\n",
    "<img src=\"img/paper-mcts.png\" style=\"width: 700px;\" >\n",
    "\n",
    "Before we get into UCT and Random Rollout, it is important to remind ourselves of the two fundamental concepts that MCTS is build upon: the value of an action may be approximated by using random simulation, and that these values may be used efficiently to adjust the policy towards a best-first strategy. \n",
    "\n",
    "### Upper Confidence Bound for Trees (UCT)\n",
    "Generally speaking Upper Confidence Bound for Trees (UCT) refers to an MCTS algorithm that utilizes some UCB variant in the tree selection policy. In our case, we will use UCB1 with an added weighting factor as our selection policy. This means that during our selection phase, we select the node with the best UCT value. Keep in mind that the UCT term already includes both exploitation and exploration incentives. Pulling things together, we get the following equation for UCT:\n",
    "\n",
    "$$UCT = \\bar{X_j} + 2 C_p \\sqrt{\\frac{2 \\text{ln} (n)}{n_j}}$$ \n",
    "\n",
    "Again $\\bar{X_j}$ is the estimated value of taking action $j$, $n$ is how times the parent node has been visited, $n_j$ is the number of times action $j$ has been taken from this state, and $C_p$ is the additional weighting factor that can toggle the relative importance of the exploitation and exploration terms. If multiple nodes are found to have the same UCT value, the tie is broken by randomly selecting one of the nodes.\n",
    "\n",
    "The UCT criteria for *Selection* and *Expansion* is what allows for asymmetric tree growth, shown in the figure below. This asymmetric tree growth is why MCTS performs better compared to brute-force exhaustive search and pruning techniques for adversarial games, such as Alpha-Beta pruning, because MCTS will focus its search on promising areas of the tree.\n",
    "\n",
    "<img src=\"img/paper-tree-growth.png\" style=\"width: 400px;\" >\n",
    "\n",
    "### Default Policy: Random Rollout \n",
    "Once an action has been selected and expanded, it is now time to perform simulation in order to retrieve an estimate for how good (or bad) this particular state is. This is where the *Default Policy* is used to get from the current state to a terminal state. Once at the terminal state, we can assess the value of this simulated version of the game and use back-propagation to share this information up the tree. As mentioned previously, we harness the power of random sampling in our *Default Policy* by employing *Random Rollout*. Random Rollout simply chooses actions randomly from the set of possible actions at a given state until a terminal state is reached. It’s that simple. It is also possible to incorporate some heuristics to guide the behavior of the rollout, but we will not consider heuristics at this time. \n",
    "\n",
    "The figure below shows an example of a random rollout for a Tic-Tac-Toe game. Since the blue circle ends up winning the game, they improve the estimated value of the first move that was made. \n",
    "\n",
    "<img src=\"img/random-tree-rollout.png\" style=\"width: 300px;\" >\n",
    "\n",
    "## Fun Example: AlphaGo Zero\n",
    "Before jumping into the implementation work of this problem set, we quickly mention a recent example of using MCTS to devise the best Go-playing AI system to date: AlphaGo Zero, to help motivate the power of MCTS. AlphaGo Zero three core components: MCTS, Neural Networks, and Reinforcement Learning. The MCTS is used in AlphaGo Zero to determine which node to expand next in the tree, just like how we will utilize MCTS in this mini-pset. However, AlphaGo Zero also uses a Neural Network to assess the value of a state and Reinforcement Learning to update the weights of this Neural Network. Also, AlphaGo Zero had the luxury of playing against itself A LOT of times, on a very high-powered machine. That said, MCTS is still the workhorse that pucks out the next move to make.\n",
    " <table><tr>\n",
    "    <td> <img src=\"img/random-go-performance.gif\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"img/random-lee-sedol.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-744c8c9c22888a18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " ---\n",
    "# Part 1: Introduction to MCTS <a id=\"part-1\"/>\n",
    "\n",
    "In the first part, you will be asked to finish the MCTS algorithm, where necessary utility functions and classes have been created but the four key steps are still waiting your implementation. After validating your code, you can see how your algorithm helps build a Gomoku AI!\n",
    "\n",
    "We have provided a Gomoku game which is steill in progress, as shown below. The black is at great advantage, and your job is to help it turn advantage into victory.\n",
    "\n",
    "<img src=\"img/gomoku_example_initial_state.png\" style=\"width: 400px;\" >\n",
    "\n",
    "---\n",
    "## Provided Classes <a id=\"part-1-provided-classes\"/>\n",
    "\n",
    "\n",
    "You are now asked to finish several key components in Monte Carlo Tree Search, namely the `select`, `expand`, `default_rollout_policy` and `backpropagate` functions. You will need to use the `Node` class we have defined.\n",
    "\n",
    "### `Node` Class\n",
    "\n",
    "A `Node` object has the following useful member variables (callable via `my_node.children`, etc):\n",
    "1. `children`: a dictionary that maps `Action` objects to `Node` objects\n",
    "1. `tot_reward`: a float of reward that has been back-propagated to this node\n",
    "1. `num_samples`: an int of number of samples that has been back-propagated to this node\n",
    "\n",
    "the following properties (callable via `my_node.state`, etc):\n",
    "1. `state`: a `State` objects corresponds to the node\n",
    "1. `parent`: a `Node` object that is the parent\n",
    "1. `unused_edges`: a list of `Action` objects that have not contributed to building child nodes\n",
    "1. `is_terminal`: a bool representing whether or not the `State` object associated with the node is a terminal state\n",
    "1. `depth`: an int of the depth of the node. The depth of a root node, which has `None` as its parent, is 1.\n",
    "1. `is_expanded`: a bool representing whether all possible actions and associated child nodes have been added to the `children` dictionary\n",
    "\n",
    "and the following methods:\n",
    "1. `add_child`(`action`): if the action is in the list `unused_edges`, then build a `Node` object `child` and add `action`: `child` to `children`, and returns `child`\n",
    "\n",
    "When making random selection from a list of variables, use the `random.choice()` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1c1dc82cdbe3ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Implement: `select` (10 points) <a id=\"selection\"/>\n",
    "<img src=\"img/paper-selection.png\" style=\"width: 700px;\" >\n",
    "\n",
    "\n",
    "Selecting which node to expand is based on choosing the node with the maximal upper confidence bound (UCB):\n",
    "$$UCB=\\frac{\\text{reward}}{\\text{number of samples of the child node}}+\\sqrt{2\\frac{\\ln({\\text{number of samples of the parent node})}}{\\text{number of samples of the child node}}}$$\n",
    "where the first term is the average reward per sample and is the exploitation term; the second term, coming from Hoeffding's inequality, will favor less unexplored choices and is the exploration term.\n",
    "\n",
    "Later when we are choosing the best strategy (not in simulation), we need to choose the action that brings the maximal reward instead of UCB, so a good practice in implementation is to make the second term vanishable:\n",
    "$$UCB=\\frac{\\text{reward}}{\\text{number of samples of the child node}}+exploration\\_const*\\sqrt{2\\frac{\\ln({\\text{number of samples of the parent node})}}{\\text{number of samples of the child node}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5146225d8db989b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select(node: Node, exploration_const: float = 1.0) -> (Action, Node):\n",
    "    \"\"\" Select the best child node based on UCB; if there are multiple\n",
    "        child nodes with the same max UCB, randomly select from the\n",
    "        list best_actions, which is to be built by you\n",
    "    :param node: The parent node\n",
    "    :param exploration_const: The exploration constant in UCB formula\n",
    "    :return: The action and the corresponding child node it leads to\n",
    "    \"\"\"\n",
    "    best_ucb = -math.inf\n",
    "    best_actions = []\n",
    "        \n",
    "    for action, child in node.children.items():\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        node_ucb = (child.tot_reward / child.num_samples\n",
    "                    + exploration_const *\n",
    "                    math.sqrt(2.0 * math.log(node.num_samples) /\n",
    "                              child.num_samples))\n",
    "        if node_ucb > best_ucb:\n",
    "            best_ucb = node_ucb\n",
    "            best_actions = [action]\n",
    "        elif node_ucb == best_ucb:\n",
    "            best_actions.append(action)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    best_action = random.choice(best_actions)\n",
    "    return best_action, node.children[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1728113dfcfcc33f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_select(select)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `expand` (10 points) <a id=\"expansion\"/>\n",
    "<img src=\"img/paper-expansion.png\" style=\"width: 700px;\" >\n",
    "Suppose you have selected a node and that node is not a terminal node, you need to randomly select an action from the list of available untaken actions, add the associated new child node to the *children* dictionary and return the child node.\n",
    "\n",
    "Hint: use the functions provided above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cac40d673a2ac43",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def expand(node: Node) -> Node:\n",
    "    \"\"\" Randomly select an untried action and create a child node based on it\n",
    "        Return the new child node\n",
    "    :param node: The parent node\n",
    "    :return: The child node\n",
    "    \"\"\"\n",
    "    if node.is_expanded:\n",
    "        raise Exception(\"Should not expand a node that has already\"\n",
    "                        \" been expanded\")\n",
    "        \n",
    "    ### BEGIN SOLUTION\n",
    "    action = random.choice(node.unused_edges)\n",
    "    return node.add_child(action)\n",
    "    ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d6f102ff29b07b2b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_expand(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `default_rollout_policy` (10 points) <a id=\"rollout-policy\"/>\n",
    "<img src=\"img/paper-simulation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "\n",
    "The default rollout policy is to keep randomly selecting an action from the available ones until a terminal state is reached.\n",
    "A State object has a property is_terminal to represent whether a it is a terminal state, another property possible_actions which is a list of Action objects, and a member method execute_action which takes an Action object as input and returns a copy of a new state.\n",
    "Hint: use random.choice to randomly select an action from a list of actions.\n",
    "\n",
    "\n",
    "### Default Rollout Policy\n",
    "\n",
    "\n",
    "TODO -- Finish Description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04f2b4885acdea15",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def default_rollout_policy(state: State) -> float:\n",
    "    \"\"\" The default policy for simulation is to randomly (uniform distribution)\n",
    "        select an action to update the state and repeat the simulation until\n",
    "        a terminal state is reached\n",
    "    :param state: The starting state\n",
    "    :return: The reward at the terminal state\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while not state.is_terminal:\n",
    "        action = random.choice(state.possible_actions)\n",
    "        state = state.execute_action(action)\n",
    "    return state.reward\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c3db0bef45352e1",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_default_rollout_policy(default_rollout_policy)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement: `backpropagate` (10 points) <a id=\"backpropagate\"/>\n",
    "<img src=\"img/paper-back-propagation.png\" style=\"width: 700px;\" >\n",
    "\n",
    "\n",
    "After simulation is performed, you have the reward of the terminal state, and the next thing is to update the visit counts and rewards for relevant nodes.\n",
    "\n",
    "Hint: `Node` objects have a member property `parent`; older visit count and reward are stored in public member variables `num_samples` and `tot_reward` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20127172b9eb7705",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backpropagate(node: Node, reward: float = 0.0) -> None:\n",
    "    \"\"\" Propagate the reward and sample count from the leaf node\n",
    "        back all the way to the root node (the node with None as parent)\n",
    "    :param node: The node where simulation starts\n",
    "    :param reward: The reward of the terminal state in the simulation\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    while node is not None:\n",
    "        node.num_samples += 1\n",
    "        node.tot_reward += reward\n",
    "        node = node.parent\n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e77f09e169ac4a3e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_backpropagate(expand)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Written Question: *MCTS with Heuristics* (10 points) <a id=\"heuristics\"/>\n",
    "\n",
    "Now you have implemented the MCTS algorithm, you should be ready to use it run it on the Gomoku problem. Note that the execution might take 3-5 minutes. To ensure that the simulation always terminates in a reasonable number of steps and to make sure that the results observed by everyone are similar, we have fixed the random seed.\n",
    "\n",
    "Does the result look good? Start thinking about why (you do not need to answer it right now).\n",
    "\n",
    "Which strategy generates a better result? Explain why. Could you come up with some potential methods to improve the performance of MCTS in Gomoku games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d637804d86c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate_with_black_sample_arbitrarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonteCarloSearchTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/pucan/GitHub Repos/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36msimulate_with_black_sample_arbitrarily\u001b[0;34m(mcts_class)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     (gomoku_example_simulate(mcts_class, black_heuristics=False,\n\u001b[0;32m---> 71\u001b[0;31m                              white_heuristics=True, seed=1000).visualize())\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pucan/GitHub Repos/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36mgomoku_example_simulate\u001b[0;34m(mcts_class, black_heuristics, white_heuristics, samples_per_step, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblack_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Black goes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mblack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_for_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-97da18a4813a>\u001b[0m in \u001b[0;36msearch_for_actions\u001b[0;34m(self, search_depth)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mexecute_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_tree_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pucan/GitHub Repos/mcts/pset/mcts.py\u001b[0m in \u001b[0;36mexecute_round\u001b[0;34m(root, rollout_policy, max_tree_depth)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expand' is not defined"
     ]
    }
   ],
   "source": [
    "simulate_with_black_sample_arbitrarily(MonteCarloSearchTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1051e1007af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate_with_black_sample_neighborhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonteCarloSearchTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36msimulate_with_black_sample_neighborhood\u001b[0;34m(mcts_class)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \"\"\"\n\u001b[1;32m     78\u001b[0m     (gomoku_example_simulate(mcts_class, black_heuristics=True,\n\u001b[0;32m---> 79\u001b[0;31m                              white_heuristics=True, seed=500).visualize())\n\u001b[0m",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/gomoku_example.py\u001b[0m in \u001b[0;36mgomoku_example_simulate\u001b[0;34m(mcts_class, black_heuristics, white_heuristics, samples_per_step, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblack_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Black goes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mblack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_for_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mblack_mcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/mcts.py\u001b[0m in \u001b[0;36msearch_for_actions\u001b[0;34m(self, search_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mexecute_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_tree_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcur_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/College/16.412/mcts-project/mcts/pset/mcts.py\u001b[0m in \u001b[0;36mexecute_round\u001b[0;34m(root, rollout_policy, max_tree_depth)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_expanded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_tree_depth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0msimulation_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_tree_depth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expand' is not defined"
     ]
    }
   ],
   "source": [
    "simulate_with_black_sample_neighborhood(MonteCarloSearchTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d4021b380fffb2a4",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: MCTS for Collaboration <a id=\"part-2\"/>\n",
    "<img src=\"img/auv-random-walk.png\" style=\"width: 400px;\" >\n",
    "\n",
    "\n",
    "In part 1, you have implemented the MCTS algorithm; in part 2, you will learn how to model a problem which can call that algorithm\n",
    "\n",
    "---\n",
    "## AUV Reward Collection Problem\n",
    "Suppose you are given 3 AUVs and your task is to use them to explore valuable locations of an underwater area, which has been spatially discretized below:\n",
    "\n",
    "<img src=\"img/maze_example_1.png\" style=\"width: 600px;\" >\n",
    "\n",
    "The time is also discretized into 15 steps. At each time step, an AUV can move to one of the 4 neighboring locations as long as it is not an obstacle (the red tiles in the figure). The radius of blue circles represent the information values of different locations: the larger radius corresponds to 3, and the smaller represents 1.\n",
    "\n",
    "We refer to this example as Maze Example 1. The task is to model this problem so the algorithm developed in Part 1 can be applied (and you will know how we implemented the Gomoku game because they are quite similar).\n",
    "\n",
    "\n",
    "---\n",
    "## Provided Classes <a id=\"part-2-provided-classes\"/>\n",
    "\n",
    "We have provided a class `UnfinishedMazaState`, and your job is to finish building the class `MazeState`. To be more specific, you need to implement 4 key properties/methods, namely `is_terminal`, `reward`, `possible_actions` and `take_action`. You may find the following information useful.\n",
    "\n",
    "### `MazeAction` Class\n",
    "1. Can be constructed using `MazeAction(agent_index: int, position: (int, int)`, where `agent_index` indicates which AUV should move, and position is a tuple representing where it is moving to\n",
    "1. Has properties `agent_index` and `position`\n",
    "\n",
    "### `MazeEnvironment` Class\n",
    "1. Has property `rewards`, whcih is a dictionary mapping position (int, int) to reward value (float)\n",
    "2. Has property `obstacles`, which is a set of position (int, int)\n",
    "\n",
    "### `UnfinishedMazeState` Class\n",
    "1. Has a method `is_in_range` that takes (int, int) as input and returns a bool, indicating whether a position is inside the problem domain\n",
    "1. Has a property `paths` - \\[`path_0`, `path_1`, `path_2`, $\\ldots$\\]. Each `path_i` is a list of position tuples (int, int); the last tuple is current position of agent $i$. This records the trajectories of all agents\n",
    "1. Has a property `time_remains` which is an int object\n",
    "1. Has a property `visited` which is a set of visited position tuples (int, int)\n",
    "1. Has a property `turn` - an int object indicating which agent should move\n",
    "1. Has a property `switch_agent` which will properly update `turn` so the next agent can move; if all agents have moved for a round, then the `time_remains` property would be updated\n",
    "1. Has a property `environment`, which is a `MazeEnvironment` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement: `Reward`, `is_terminal`, `possible_actions`, `take_action` (10 points each) <a id =\"maze\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(UnfinishedMazeState):\n",
    "\n",
    "    def __init__(self, environment: MazeEnvironment, time_remains: int = 10):\n",
    "        \"\"\" Create a state of the AUV reward-collection game\n",
    "        \"\"\"\n",
    "        super(UnfinishedMazeState, self).__init__(environment, time_remains)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def reward(self) -> float:\n",
    "        \"\"\" The total reward at the current state is value of\n",
    "            rewards that have been visited by agents\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        reward = 0.0\n",
    "        for target_position in self._environment.rewards:\n",
    "            if target_position in self.visited:\n",
    "                reward += self._environment.rewards[target_position]\n",
    "        return reward\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\" The only terminal condition is no time remains\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.time_remains <= 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "        \n",
    "    @property\n",
    "    def possible_actions(self) -> list:\n",
    "        \"\"\" The possible actions based on the current state\n",
    "            Note that you are only moving a single agent\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        i, j = self._paths[self._turn][-1]\n",
    "        actions = [MazeAction(self._turn, (i + 1, j)),\n",
    "                   MazeAction(self._turn, (i - 1, j)),\n",
    "                   MazeAction(self._turn, (i, j + 1)),\n",
    "                   MazeAction(self._turn, (i, j - 1))]\n",
    "        return [MazeAction(self._turn, action.position) for action in actions if\n",
    "                self.is_in_range(action.position) and\n",
    "                action.position not in self._environment.obstacles]\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    def take_action(self, action: MazeAction) -> \"MazeState\":\n",
    "        \"\"\" Execute the action based on the current state\n",
    "            You can assume that the action is always valid\n",
    "        :param action: The action\n",
    "        :return: The state after being updated\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self._paths[self._turn].append(action.position)\n",
    "        self._turn = (self._turn + 1) % len(self._paths)\n",
    "        if self._turn == 0:  # When all agents have taken a turn of actions\n",
    "            self._time_remains -= 1\n",
    "        return self\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your code is validated, you can run the simulation. Note that the simulation would take several minutes and we have fixed the random seed.\n",
    "\n",
    "You can check the check the correctness of your code in incremental steps \n",
    "TODO -- mention that tests assume order of implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_1() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-14ddfb3b1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_1() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_1()).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1315a5a1f55ef576",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_reward(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5a1a4a7e0465456",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_is_terminal(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-727963e36b39a45f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_possible_actions(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-53bbf722c18cb613",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_take_action(MazeState)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Written Question: *Optimal Strategy* (5 points) <a id=\"written-optimal\"/>\n",
    "\n",
    "What is the optimal strategy? You only need to conceptually describe it. There is no need to enter the exact moves for each time steps.\n",
    "\n",
    "For example:\n",
    "> The AUV starting at (13, 8) should first collect the reward at (14, 10), (14, 11), and then move towards the reward at (15, 8)\n",
    "> The AUV starting at (7, 7) should move towards the reward at (11, 2)\n",
    "> The AUV starting at (12, 12) should move towards the reward at (4, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25cdfc53a4e11f2d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508fc0042443dd14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, run the simulation. The execution will again take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40f6bdd9ff826f63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_1() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-14ddfb3b1513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_1() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_1()).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e39854823691bcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Written Question: *Maze Comparison* (5 points) <a id=\"written-comparison\"/>\n",
    "\n",
    "\n",
    "When using the default rollout policy, do you think MCTS will perform equally well in Maze Example 2 as in Maze Example 1? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ae33ce4e0c56bfb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maze_example_2() missing 1 required positional argument: 'state_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-eef44224e8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_example_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: maze_example_2() missing 1 required positional argument: 'state_class'"
     ]
    }
   ],
   "source": [
    "simulate(maze_example_2()).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b07cc8047b2ed26e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af0a8bf29b28875b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## References \n",
    "[1] Browne et. al. 2012, *A Survey of Monte Carlo Tree Search Methods*, IEEE.\n",
    "\n",
    "[2] Auer et. al. 2001, *Finite-time Analysis of the Multiarmed Bandit Problem*, International Conference on Machine Learning.\n",
    "\n",
    "TODO -- add more references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
